[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "",
    "text": "Ocean wave prediction is used for maritime safety, wave energy conversion, and coastal engineering applications. This research explores deep learning approaches for predicting ocean surface elevation time-series using historical buoy measurements. While traditional wave analysis relies on statistical parameters including significant wave height (H_{m_0}) and energy period (T_e), many applications could benefit from more accurate wave-by-wave predictions of surface elevation.\nThe need for accurate wave prediction is particularly evident in wave energy applications, where Ringwood (2020) highlights challenges in control system optimization that depend on reliable wave forecasts. Abdelkhalik et al. (2016) demonstrates how wave predictions enable real-time optimization of energy extraction, showing that accurate forecasting directly impacts system performance and efficiency.\nThis project addresses the fundamental need for accurate near real-time wave prediction by developing deep learning models to forecast three-dimensional surface elevation time-series, focusing on maintaining both prediction accuracy and computational efficiency through models trained on previously collected measurements.\n\n\nThe study utilizes surface elevation wave measurements from the Coastal Data Information Program (CDIP) focusing on two strategic United States locations, Kaneohe Bay, Hawaii and Nags Head North Carolina. These locations were chosen because they have many years of realtime measurement and the sites have significant seasonal variations in wave conditions.\nFigure 1 shows the Kaneohe Bay, Hawaii buoy location (CDIP 225) This buoy is located within the U.S. Navy’s Wave Energy Test Site (WETS) (Coastal Data Information Program 2023a). This deep water deployment at 84m depth experiences a mixed wave climate with trade wind waves, North Pacific swell, and South Pacific swell. The site generally maintains consistent wave conditions due to trade wind dominance.\n\n\nCode\nimport folium\n\ndef create_location_map(latitude, longitude, label, zoom_level=8):\n    location_map = folium.Map(location=[latitude, longitude], zoom_start=zoom_level)\n\n    folium.Marker(\n        [latitude, longitude],\n        popup=label,\n        icon=folium.Icon(color=\"red\", icon=\"info-sign\"),\n    ).add_to(location_map)\n\n    return location_map\n\n\n\n\nCode\nwets_map = create_location_map(\n    latitude=21.47740,\n    longitude=-157.75684,\n    label='CDIP 225 - WETS Hawaii'\n)\nwets_map\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 1: Map of CDIP Buoy 225 Location at the Wave Energy Test Site, Kaneohe Bay, Oahu, HI\n\n\n\n\nFigure 2 shows the Nags Head, North Carolina buoy location (CDIP 243) (Coastal Data Information Program 2023b). This buoy is located near the Jennettes’s Pier Wave Energy Test Center. This site, at an intermediate water depth of 21m, experiences primarily wind-driven wave conditions. The wave climate is highly variable due to the influence of Cape Hatteras weather systems, with conditions ranging from calm seas to severe storm events including tropical cyclones.\n\n\nCode\nnags_head_map = create_location_map(\n    latitude= 36.00150,\n    longitude=-75.42090,\n    label='CDIP 243 - Nags Head NC'\n)\nnags_head_map\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 2: Map of CDIP Buoy 243 Location in Nags Head, NC\n\n\n\n\nTable 1 compares the characteristics of the chosen measurement locations.\n\n\n\n\nTable 1: CDIP 225 vs CDIP 243 site comparisons\n\n\n\n\n\nCharacteristic\nKaneohe Bay, Hawaii (CDIP 225)\nNags Head, NC (CDIP 243)\n\n\n\n\nWater Depth\nDeep water (84m)\nIntermediate water (21m)\n\n\nWave Climate\nMixed: trade wind waves, North & South Pacific swell\nPrimarily wind-driven\n\n\nWave Conditions\nGenerally consistent due to trade winds\nHighly variable due to Cape Hatteras weather\n\n\nWeather Events\nSeasonal variations from Pacific storms\nTropical cyclones, severe storms\n\n\nResearch Site\nWave Energy Test Site (WETS)\nJennette’s Pier Wave Energy Test Center\n\n\nCDIP Link\nCDIP 225\nCDIP 243\n\n\nData Record\nAvailable since 2012\nAvailable since 2017\n\n\n\n\n\n\n\n\n\n\nSurface elevation measurements are collected using Datawell Waverider DWR-MkIII buoys, detailed in Datawell BV (2006). These specialized oceanographic instruments capture three-dimensional displacement measurements at a sampling frequency of 1.28 Hz, providing high-resolution data of vertical, northward, and eastward wave motion. All measurements undergo CDIP’s standardized quality control and processing pipeline before being archived and made available for analysis.\n\n\n\n\n\n\nFigure 3: CDIP Buoy 204 Deployed in Lower Cook Inlet Alaska. Photo from AOOS\n\n\n\n\n\nAs illustrated in Figure 4, the buoy’s movement is tracked in a three-dimensional reference frame, measuring displacements in vertical (Z), east-west (X), and north-south (Y) directions. Our prediction task focuses on forecasting these three displacement components in real-time as measurement data streams from the buoy. This multivariate time series prediction approach uses historical measurements of all three displacement components to forecast their future values over specified time horizons, providing a comprehensive representation of wave motion at each location.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport seaborn as sns\nsns.set_theme()\n\n# Get a pleasing color palette\n# colors = sns.color_palette(\"husl\", 3)  # Using husl for distinct but harmonious colors\ncolors = sns.color_palette()\nx_color = colors[0]\ny_color = colors[1]\nz_color = colors[2]\n\n# Create figure\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\n\n# Create sphere\nu = np.linspace(0, 2 * np.pi, 100)\nv = np.linspace(0, np.pi, 100)\nx = 0.4 * np.outer(np.cos(u), np.sin(v))\ny = 0.4 * np.outer(np.sin(u), np.sin(v))\nz = 0.4 * np.outer(np.ones(np.size(u)), np.cos(v))\n\n# Plot semi-transparent sphere\nax.plot_surface(x, y, z, color='orange', alpha=0.3)\n\n# Plot axes through sphere center\nlength = 0.6\nax.plot([-length, length], [0, 0], [0, 0], color=x_color, linewidth=2, label='X (East/West)')\nax.plot([0, 0], [-length, length], [0, 0], color=y_color, linewidth=2, label='Y (True North/South)')\nax.plot([0, 0], [0, 0], [-length, length], color=z_color, linewidth=2, label='Z (Vertical)')\n\n# Add arrows at the ends\narrow_length = 0.1\n# X axis arrows\nax.quiver(length, 0, 0, arrow_length, 0, 0, color=x_color, arrow_length_ratio=0.3)\nax.quiver(-length, 0, 0, -arrow_length, 0, 0, color=x_color, arrow_length_ratio=0.3)\n# Y axis arrows\nax.quiver(0, length, 0, 0, arrow_length, 0, color=y_color, arrow_length_ratio=0.3)\nax.quiver(0, -length, 0, 0, -arrow_length, 0, color=y_color, arrow_length_ratio=0.3)\n# Z axis arrows\nax.quiver(0, 0, length, 0, 0, arrow_length, color=z_color, arrow_length_ratio=0.3)\nax.quiver(0, 0, -length, 0, 0, -arrow_length, color=z_color, arrow_length_ratio=0.3)\n\n\n# Set equal aspect ratio\nax.set_box_aspect([1,1,1])\n\n# Set axis limits\nlimit = 0.55\nax.set_xlim([-limit, limit])\nax.set_ylim([-limit, limit])\nax.set_zlim([-limit, limit])\n\n# Add grid\nax.grid(True, alpha=0.3)\n\n# Add axis labels with matching colors\nax.set_xlabel('East/West Displacement (X) [m]', color=x_color, weight='bold', fontsize=18)\nax.set_ylabel('True North/South Displacement (Y) [m]', color=y_color, weight='bold', fontsize=18)\nax.set_zlabel('Vertical Displacement (Z) [m]', color=z_color, weight='bold', fontsize=18)\n\n# Adjust view angle\nax.view_init(elev=20, azim=180 - 45)\n\n# Set background color to white\nax.set_facecolor('white')\nfig.patch.set_facecolor('white')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Directional Reference Frame of Datawell Waverider DWR-MkIII Buoy\n\n\n\n\n\n\n\n\n\nThis project leverages both Long Short-Term Memory (LSTM) networks and Transformer architectures to predict ocean surface elevation measurements. LSTMs, first introduced by Hochreiter and Schmidhuber (1997), have demonstrated success in temporal sequence learning through their ability to capture long-term dependencies. The Transformer architecture, developed by Vaswani et al. (2023), offers an alternative approach using self-attention mechanisms to process sequential data without recurrence.\nPrevious work in ocean wave forecasting has shown promise using neural network approaches. Mandal and Prabaharan (2006) demonstrated effective wave height prediction using recurrent neural networks, while Kumar, Savitha, and Mamun (2017) explored sequential learning algorithms for regional wave height forecasting. Building on these foundations, our approach implements both LSTM and Transformer models using the PyTorch framework by Ansel et al. (2024), and the LSTM Module described by Sak, Senior, and Beaufays (2014), allowing for direct comparison of their performance in predicting three-dimensional surface elevation time series.\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    EDA[\"&lt;div style='line-height:1.0;'&gt;Exploratory&lt;br&gt;Data&lt;br&gt;Analysis&lt;/div&gt;\"]\n    --&gt; Preprocess[\"&lt;div style='line-height:1.0;'&gt;Data&lt;br&gt;Preprocessing&lt;/div&gt;\"]\n    Preprocess --&gt; ModelDev{\"&lt;div style='line-height:1.0;'&gt;Model&lt;br&gt;Development&lt;/div&gt;\"}\n    ModelDev --&gt; LSTM[\"&lt;div style='line-height:1.0;'&gt;LSTM&lt;br&gt;Architecture&lt;/div&gt;\"]\n    ModelDev --&gt; Transformer[\"&lt;div style='line-height:1.0;'&gt;Transformer&lt;br&gt;Architecture&lt;/div&gt;\"]\n    LSTM --&gt; ModelEval{\"&lt;div style='line-height:1.0;'&gt;Model&lt;br&gt;Evaluation&lt;/div&gt;\"}\n    Transformer --&gt; ModelEval\n    ModelEval --&gt; Tune[\"&lt;div style='line-height:1.0;'&gt;Hyperparameter&lt;br&gt;Tuning&lt;/div&gt;\"]\n    Tune --&gt; ModelEval\n    ModelEval --&gt; BestModel[\"&lt;div style='line-height:1.0;'&gt;Select Best&lt;br&gt;Model&lt;/div&gt;\"]\n    BestModel --&gt; Deploy[\"&lt;div style='line-height:1.0;'&gt;Final Model&lt;br&gt;Deployment&lt;/div&gt;\"]\n    Deploy --&gt; Validate[\"&lt;div style='line-height:1.0;'&gt;Site&lt;br&gt;Validation&lt;/div&gt;\"]\n\n\n\n\nFigure 5: Wave Prediction Modeling Workflow\n\n\n\n\n\n\nThe workflow in Figure 5 is a systematic deep learning workflow optimized for wave prediction modeling. Beginning with exploratory data analysis of CDIP buoy measurements, the data undergoes preprocessing including normalization and temporal windowing. The model development phase explores multiple neural network architectures in parallel, followed by rigorous evaluation and hyperparameter tuning. The final model undergoes cross-site validation to assess its generalization capabilities across different ocean environments."
  },
  {
    "objectID": "index.html#data-sources",
    "href": "index.html#data-sources",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "",
    "text": "The study utilizes surface elevation wave measurements from the Coastal Data Information Program (CDIP) focusing on two strategic United States locations, Kaneohe Bay, Hawaii and Nags Head North Carolina. These locations were chosen because they have many years of realtime measurement and the sites have significant seasonal variations in wave conditions.\nFigure 1 shows the Kaneohe Bay, Hawaii buoy location (CDIP 225) This buoy is located within the U.S. Navy’s Wave Energy Test Site (WETS) (Coastal Data Information Program 2023a). This deep water deployment at 84m depth experiences a mixed wave climate with trade wind waves, North Pacific swell, and South Pacific swell. The site generally maintains consistent wave conditions due to trade wind dominance.\n\n\nCode\nimport folium\n\ndef create_location_map(latitude, longitude, label, zoom_level=8):\n    location_map = folium.Map(location=[latitude, longitude], zoom_start=zoom_level)\n\n    folium.Marker(\n        [latitude, longitude],\n        popup=label,\n        icon=folium.Icon(color=\"red\", icon=\"info-sign\"),\n    ).add_to(location_map)\n\n    return location_map\n\n\n\n\nCode\nwets_map = create_location_map(\n    latitude=21.47740,\n    longitude=-157.75684,\n    label='CDIP 225 - WETS Hawaii'\n)\nwets_map\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 1: Map of CDIP Buoy 225 Location at the Wave Energy Test Site, Kaneohe Bay, Oahu, HI\n\n\n\n\nFigure 2 shows the Nags Head, North Carolina buoy location (CDIP 243) (Coastal Data Information Program 2023b). This buoy is located near the Jennettes’s Pier Wave Energy Test Center. This site, at an intermediate water depth of 21m, experiences primarily wind-driven wave conditions. The wave climate is highly variable due to the influence of Cape Hatteras weather systems, with conditions ranging from calm seas to severe storm events including tropical cyclones.\n\n\nCode\nnags_head_map = create_location_map(\n    latitude= 36.00150,\n    longitude=-75.42090,\n    label='CDIP 243 - Nags Head NC'\n)\nnags_head_map\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 2: Map of CDIP Buoy 243 Location in Nags Head, NC\n\n\n\n\nTable 1 compares the characteristics of the chosen measurement locations.\n\n\n\n\nTable 1: CDIP 225 vs CDIP 243 site comparisons\n\n\n\n\n\nCharacteristic\nKaneohe Bay, Hawaii (CDIP 225)\nNags Head, NC (CDIP 243)\n\n\n\n\nWater Depth\nDeep water (84m)\nIntermediate water (21m)\n\n\nWave Climate\nMixed: trade wind waves, North & South Pacific swell\nPrimarily wind-driven\n\n\nWave Conditions\nGenerally consistent due to trade winds\nHighly variable due to Cape Hatteras weather\n\n\nWeather Events\nSeasonal variations from Pacific storms\nTropical cyclones, severe storms\n\n\nResearch Site\nWave Energy Test Site (WETS)\nJennette’s Pier Wave Energy Test Center\n\n\nCDIP Link\nCDIP 225\nCDIP 243\n\n\nData Record\nAvailable since 2012\nAvailable since 2017"
  },
  {
    "objectID": "index.html#data-acquisition-hardware",
    "href": "index.html#data-acquisition-hardware",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "",
    "text": "Surface elevation measurements are collected using Datawell Waverider DWR-MkIII buoys, detailed in Datawell BV (2006). These specialized oceanographic instruments capture three-dimensional displacement measurements at a sampling frequency of 1.28 Hz, providing high-resolution data of vertical, northward, and eastward wave motion. All measurements undergo CDIP’s standardized quality control and processing pipeline before being archived and made available for analysis.\n\n\n\n\n\n\nFigure 3: CDIP Buoy 204 Deployed in Lower Cook Inlet Alaska. Photo from AOOS\n\n\n\n\n\nAs illustrated in Figure 4, the buoy’s movement is tracked in a three-dimensional reference frame, measuring displacements in vertical (Z), east-west (X), and north-south (Y) directions. Our prediction task focuses on forecasting these three displacement components in real-time as measurement data streams from the buoy. This multivariate time series prediction approach uses historical measurements of all three displacement components to forecast their future values over specified time horizons, providing a comprehensive representation of wave motion at each location.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport seaborn as sns\nsns.set_theme()\n\n# Get a pleasing color palette\n# colors = sns.color_palette(\"husl\", 3)  # Using husl for distinct but harmonious colors\ncolors = sns.color_palette()\nx_color = colors[0]\ny_color = colors[1]\nz_color = colors[2]\n\n# Create figure\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\n\n# Create sphere\nu = np.linspace(0, 2 * np.pi, 100)\nv = np.linspace(0, np.pi, 100)\nx = 0.4 * np.outer(np.cos(u), np.sin(v))\ny = 0.4 * np.outer(np.sin(u), np.sin(v))\nz = 0.4 * np.outer(np.ones(np.size(u)), np.cos(v))\n\n# Plot semi-transparent sphere\nax.plot_surface(x, y, z, color='orange', alpha=0.3)\n\n# Plot axes through sphere center\nlength = 0.6\nax.plot([-length, length], [0, 0], [0, 0], color=x_color, linewidth=2, label='X (East/West)')\nax.plot([0, 0], [-length, length], [0, 0], color=y_color, linewidth=2, label='Y (True North/South)')\nax.plot([0, 0], [0, 0], [-length, length], color=z_color, linewidth=2, label='Z (Vertical)')\n\n# Add arrows at the ends\narrow_length = 0.1\n# X axis arrows\nax.quiver(length, 0, 0, arrow_length, 0, 0, color=x_color, arrow_length_ratio=0.3)\nax.quiver(-length, 0, 0, -arrow_length, 0, 0, color=x_color, arrow_length_ratio=0.3)\n# Y axis arrows\nax.quiver(0, length, 0, 0, arrow_length, 0, color=y_color, arrow_length_ratio=0.3)\nax.quiver(0, -length, 0, 0, -arrow_length, 0, color=y_color, arrow_length_ratio=0.3)\n# Z axis arrows\nax.quiver(0, 0, length, 0, 0, arrow_length, color=z_color, arrow_length_ratio=0.3)\nax.quiver(0, 0, -length, 0, 0, -arrow_length, color=z_color, arrow_length_ratio=0.3)\n\n\n# Set equal aspect ratio\nax.set_box_aspect([1,1,1])\n\n# Set axis limits\nlimit = 0.55\nax.set_xlim([-limit, limit])\nax.set_ylim([-limit, limit])\nax.set_zlim([-limit, limit])\n\n# Add grid\nax.grid(True, alpha=0.3)\n\n# Add axis labels with matching colors\nax.set_xlabel('East/West Displacement (X) [m]', color=x_color, weight='bold', fontsize=18)\nax.set_ylabel('True North/South Displacement (Y) [m]', color=y_color, weight='bold', fontsize=18)\nax.set_zlabel('Vertical Displacement (Z) [m]', color=z_color, weight='bold', fontsize=18)\n\n# Adjust view angle\nax.view_init(elev=20, azim=180 - 45)\n\n# Set background color to white\nax.set_facecolor('white')\nfig.patch.set_facecolor('white')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Directional Reference Frame of Datawell Waverider DWR-MkIII Buoy"
  },
  {
    "objectID": "index.html#sec-dl-overview",
    "href": "index.html#sec-dl-overview",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "",
    "text": "This project leverages both Long Short-Term Memory (LSTM) networks and Transformer architectures to predict ocean surface elevation measurements. LSTMs, first introduced by Hochreiter and Schmidhuber (1997), have demonstrated success in temporal sequence learning through their ability to capture long-term dependencies. The Transformer architecture, developed by Vaswani et al. (2023), offers an alternative approach using self-attention mechanisms to process sequential data without recurrence.\nPrevious work in ocean wave forecasting has shown promise using neural network approaches. Mandal and Prabaharan (2006) demonstrated effective wave height prediction using recurrent neural networks, while Kumar, Savitha, and Mamun (2017) explored sequential learning algorithms for regional wave height forecasting. Building on these foundations, our approach implements both LSTM and Transformer models using the PyTorch framework by Ansel et al. (2024), and the LSTM Module described by Sak, Senior, and Beaufays (2014), allowing for direct comparison of their performance in predicting three-dimensional surface elevation time series."
  },
  {
    "objectID": "index.html#project-workflow",
    "href": "index.html#project-workflow",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "",
    "text": "flowchart LR\n    EDA[\"&lt;div style='line-height:1.0;'&gt;Exploratory&lt;br&gt;Data&lt;br&gt;Analysis&lt;/div&gt;\"]\n    --&gt; Preprocess[\"&lt;div style='line-height:1.0;'&gt;Data&lt;br&gt;Preprocessing&lt;/div&gt;\"]\n    Preprocess --&gt; ModelDev{\"&lt;div style='line-height:1.0;'&gt;Model&lt;br&gt;Development&lt;/div&gt;\"}\n    ModelDev --&gt; LSTM[\"&lt;div style='line-height:1.0;'&gt;LSTM&lt;br&gt;Architecture&lt;/div&gt;\"]\n    ModelDev --&gt; Transformer[\"&lt;div style='line-height:1.0;'&gt;Transformer&lt;br&gt;Architecture&lt;/div&gt;\"]\n    LSTM --&gt; ModelEval{\"&lt;div style='line-height:1.0;'&gt;Model&lt;br&gt;Evaluation&lt;/div&gt;\"}\n    Transformer --&gt; ModelEval\n    ModelEval --&gt; Tune[\"&lt;div style='line-height:1.0;'&gt;Hyperparameter&lt;br&gt;Tuning&lt;/div&gt;\"]\n    Tune --&gt; ModelEval\n    ModelEval --&gt; BestModel[\"&lt;div style='line-height:1.0;'&gt;Select Best&lt;br&gt;Model&lt;/div&gt;\"]\n    BestModel --&gt; Deploy[\"&lt;div style='line-height:1.0;'&gt;Final Model&lt;br&gt;Deployment&lt;/div&gt;\"]\n    Deploy --&gt; Validate[\"&lt;div style='line-height:1.0;'&gt;Site&lt;br&gt;Validation&lt;/div&gt;\"]\n\n\n\n\nFigure 5: Wave Prediction Modeling Workflow\n\n\n\n\n\n\nThe workflow in Figure 5 is a systematic deep learning workflow optimized for wave prediction modeling. Beginning with exploratory data analysis of CDIP buoy measurements, the data undergoes preprocessing including normalization and temporal windowing. The model development phase explores multiple neural network architectures in parallel, followed by rigorous evaluation and hyperparameter tuning. The final model undergoes cross-site validation to assess its generalization capabilities across different ocean environments."
  },
  {
    "objectID": "index.html#displacement-measurement",
    "href": "index.html#displacement-measurement",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.1 Displacement Measurement",
    "text": "2.1 Displacement Measurement\nOcean wave measurements from CDIP buoys utilize sophisticated sensor arrays including accelerometers, magnetometers, and GPS sensors to track three-dimensional wave motion Datawell BV (2006). These instruments output vertical displacement (commonly referred to as surface elevation), along with northward and eastward displacements. Figure Figure 6 illustrates these three displacement components using a 30-minute sample from CDIP 225, demonstrating the typical wave motion captured by a Datawell Waverider buoy.\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme()\ndf = pd.read_parquet(\"../data/a2_std_partition/station_number=0225/year=2017/month=11/day=11/hour=11/minute=00/data_20171111_1100.parquet\")\n\ndf['vert_displacement_meters'].plot(figsize=(9, 2), linewidth=0.85, xlabel=\"Time\", ylabel=\"Vertical\\nDisplacement [m]\")\nplt.show()\n\ndf['north_displacement_meters'].plot(figsize=(9, 2), linewidth=0.85, xlabel=\"Time\", ylabel=\"North/South\\nDisplacement [m]\")\nplt.show()\n\ndf['east_displacement_meters'].plot(figsize=(9, 2), linewidth=0.85, xlabel=\"Time\", ylabel=\"East/West\\nDisplacement [m]\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Vertical (Z) Displacement\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) North/South (Y) Displacement\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) East/West (X) Displacement\n\n\n\n\n\n\n\nFigure 6: CDIP 225 30 minutes of Displacement - November 11, 2017 @ 11 am. Data from Coastal Data Information Program (2023a)\n\n\n\n\nIn Figure 7, a detailed view of approximately 90 seconds of the same time series reveals the fine-scale structure of wave motion. While the overall pattern exhibits typical wave behavior, the vertical displacement shows distinct non-sinusoidal characteristics, particularly during directional transitions. These abrupt changes in vertical motion highlight the complex, non-linear nature of real ocean waves compared to idealized wave forms.\n\n\nCode\nsns.set_theme()\ndf = pd.read_parquet(\"../data/a2_std_partition/station_number=0225/year=2017/month=11/day=11/hour=11/minute=00/data_20171111_1100.parquet\")\n\nend_index = int(288 / 2) # 2304 / 2 / 2 / 2 - ~3 minutes / 2 = 90 seconds\n\ndf['vert_displacement_meters'].iloc[:end_index].plot(figsize=(9, 2), linewidth=0.85, xlabel=\"Time\", ylabel=\"Vertical\\nDisplacement [m]\", marker=\".\", markersize=2)\nplt.show()\n\ndf['north_displacement_meters'].iloc[:end_index].plot(figsize=(9, 2), linewidth=0.85, xlabel=\"Time\", ylabel=\"North/South\\nDisplacement [m]\", marker=\".\", markersize=2)\nplt.show()\ndf['east_displacement_meters'].iloc[:end_index].plot(figsize=(9, 2), linewidth=0.85, xlabel=\"Time\", ylabel=\"East/West\\nDisplacement [m]\", marker=\".\", markersize=2)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Vertical (Z) Displacement\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) North/South (Y) Displacement\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) East/West (X) Displacement\n\n\n\n\n\n\n\nFigure 7: CDIP 225 1.5 minutes of Displacement - November 11, 2017 @ 11 am. Data from Coastal Data Information Program (2023a)"
  },
  {
    "objectID": "index.html#data-source-and-download",
    "href": "index.html#data-source-and-download",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.2 Data Source and Download",
    "text": "2.2 Data Source and Download\nThe displacement data used in this study was obtained from CDIP’s THREDDS data server. Data for both locations - the Wave Energy Test Site (CDIP 225) and Nags Head (CDIP 243) - is hosted on CDIP’s archive server with standardized displacement time series documentation. The data is provided in NetCDF format.\nEach data file contains three-dimensional displacement measurements sampled at 1.28 Hz, along with corresponding quality control flags. The raw NetCDF files were processed using xarray by Hoyer and Joseph (2017) for efficient handling of the multidimensional data. Timestamps were generated according to CDIP specifications, accounting for sampling rates and filter delays. The processed measurements were then consolidated into pandas DataFrames, developed by The pandas development team (n.d.), and stored in parquet format for efficient access during model development.\n\n\n\n\n\nListing 1: CDIP Download Implementation\n\n\nfrom pathlib import Path\n\nimport requests\nimport xarray as xr\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nsns.set_theme()\n\n# NAGS HEAD, NC - 243\n# station_number = \"243\"\n# KANEOHE BAY, WETS, HI - 225\nstation_number = \"225\"\n# 1.28 hz * 30\nSAMPLES_PER_HALF_HOUR = 2304\n\n\ndef get_cdip_displacement_df(station_number, dataset_number):\n    fname = f\"{station_number}p1_d{dataset_number}.nc\"\n\n    nc_path = Path(f\"./data/00_raw/{fname}\").resolve()\n    print(f\"Opening {nc_path} if it exists...\")\n\n    if nc_path.exists() is False:\n        nc_url = f\"https://thredds.cdip.ucsd.edu/thredds/fileServer/cdip/archive/{station_number}p1/{fname}\"\n        print(\"Downloading\", nc_url)\n        # Download the NetCDF file using requests\n        response = requests.get(nc_url)\n        with open(nc_path, \"wb\") as f:\n            f.write(response.content)\n\n    # Open the downloaded NetCDF file with xarray\n    ds = xr.open_dataset(nc_path)\n\n    # Extract the relevant variables from the dataset\n    xdisp = ds[\"xyzXDisplacement\"]  # North/South Displacement (X)\n    ydisp = ds[\"xyzYDisplacement\"]  # East/West Displacement (Y)\n    zdisp = ds[\"xyzZDisplacement\"]  # Vertical Displacement (Z)\n    qc_flag = ds[\"xyzFlagPrimary\"]  # Quality control flag\n\n    # For some reason all of these are missing one sample. So we remove the last section\n\n    xdisp = xdisp[:-(SAMPLES_PER_HALF_HOUR)]\n    ydisp = ydisp[:-(SAMPLES_PER_HALF_HOUR)]\n    zdisp = zdisp[:-(SAMPLES_PER_HALF_HOUR)]\n    qc_flag = qc_flag[:-(SAMPLES_PER_HALF_HOUR)]\n\n    filter_delay = ds[\"xyzFilterDelay\"].values\n    start_time = ds[\"xyzStartTime\"].values  # Start time of buoy data collection\n    sample_rate = float(\n        ds[\"xyzSampleRate\"].values\n    )  # Sample rate of buoy data collection\n    sample_rate = round(sample_rate, 2)\n    print(\n        f\"Station Number: {station_number}, dataset_number: {dataset_number}, sample_rate: {sample_rate}\"\n    )\n\n    print(f\"Len xdisp: {len(xdisp)}, num 30 min sections = {(len(xdisp) + 1) / 2304}\")\n    print(f\"Filter delay: {filter_delay}\")\n\n    sample_delta_t_seconds = 1 / sample_rate\n    sample_delta_t_nanoseconds = sample_delta_t_seconds * 1e9\n    n_times = len(xdisp)\n\n    start_time_ns = start_time.astype(\"int64\")\n\n    start_time_ns = start_time.astype(\"int64\")  # Convert start_time to nanoseconds\n    # start_time_ns -= filter_delay * 1e9\n    time_increments = (\n        np.arange(n_times) * sample_delta_t_nanoseconds\n    )  # Create an array of time increments\n    times = start_time_ns + time_increments\n\n    time = pd.to_datetime(times, unit=\"ns\", origin=\"unix\", utc=True)  # type: ignore\n\n    df = pd.DataFrame(\n        {\n            \"north_displacement_meters\": xdisp,\n            \"east_displacement_meters\": ydisp,\n            \"vert_displacement_meters\": zdisp,\n            \"qc_displacement\": qc_flag,\n        },\n        index=time,\n    )\n\n    return df\n\n\nstation_number = \"225\"\nstation_number = \"243\"\ndf_1 = get_cdip_displacement_df(station_number, \"01\")\ndf_2 = get_cdip_displacement_df(station_number, \"02\")\ndf_3 = get_cdip_displacement_df(station_number, \"03\")\ndf_4 = get_cdip_displacement_df(station_number, \"04\")\n# df_5 = get_cdip_displacement_df(station_number, \"05\")\n\n# df_all = pd.concat([df_1, df_2, df_3, df_4, df_5], axis=\"index\")\ndf_all = pd.concat([df_1, df_2, df_3, df_4], axis=\"index\")\n# df_all = pd.concat([df_1, df_2, df_3], axis=\"index\")\ndf_all = df_all.sort_index()\n\ndf_all.to_parquet(f\"./data/a1_one_to_one_parquet/{station_number}_all.parquet\")\n\nprint(df_all.info())\n\nprint(f\"Successfully saved {station_number}_all.parquet!\")\n\n\n\n\n\nThe CDIP download, shown in Listing 1, demonstrates the automated download and processing pipeline. This code handles the retrieval of NetCDF files, extraction of displacement measurements, timestamp generation, and data organization into a structured format suitable for analysis and model training. Quality control flags are preserved throughout the processing pipeline to ensure data integrity."
  },
  {
    "objectID": "index.html#available-data",
    "href": "index.html#available-data",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.3 Available Data",
    "text": "2.3 Available Data\nThis section examines the temporal extent and characteristics of the available wave measurements.\n\n2.3.1 Duration\n\n\nCode\nfirst_225 = pd.read_parquet(\"../data/a2_std_partition/station_number=0225/year=2016/month=08/day=26/hour=22/minute=00/data_20160826_2200.parquet\")\nfirst_225_timestamp = first_225.index[0]\nfirst_225_timestamp\nlast_225 = pd.read_parquet(\"../data/a2_std_partition/station_number=0225/year=2024/month=09/day=11/hour=18/minute=30/data_20240911_1830.parquet\")\nlast_225_timestamp = last_225.index[-1]\nlast_225_timestamp\nfirst_243 = pd.read_parquet(\"../data/a2_std_partition/station_number=0243/year=2018/month=08/day=26/hour=15/minute=00/data_20180826_1500.parquet\")\nfirst_243_timestamp = first_243.index[0]\nfirst_243_timestamp\n\nlast_243 = pd.read_parquet(\"../data/a2_std_partition/station_number=0243/year=2023/month=07/day=12/hour=23/minute=30/data_20230712_2330.parquet\")\nlast_243_timestamp = last_243.index[-1]\nlast_243_timestamp\n\nfrom datetime import datetime\n\n# Create the data\ndata = {\n    'station': ['225', '243'],\n    'start_date': [\n        first_225_timestamp,\n        first_243_timestamp,\n    ],\n    'end_date': [\n        last_225_timestamp,\n        last_243_timestamp,\n    ]\n}\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n\n# Calculate duration\ndf['duration'] = df['end_date'] - df['start_date']\n\n# Function to format duration in human readable format\ndef format_duration(timedelta):\n    years = timedelta.days // 365\n    remaining_days = timedelta.days % 365\n    months = remaining_days // 30\n    days = remaining_days % 30\n\n    parts = []\n    if years &gt; 0:\n        parts.append(f\"{years} {'year' if years == 1 else 'years'}\")\n    if months &gt; 0:\n        parts.append(f\"{months} {'month' if months == 1 else 'months'}\")\n    if days &gt; 0:\n        parts.append(f\"{days} {'day' if days == 1 else 'days'}\")\n\n    return \", \".join(parts)\n\n# Add human readable duration\ndf['duration_human'] = df['duration'].apply(format_duration)\n\n# Format datetime columns to be more readable\ndf['start_date'] = df['start_date'].dt.strftime('%Y-%m-%d %H:%M')\ndf['end_date'] = df['end_date'].dt.strftime('%Y-%m-%d %H:%M')\n\ndf = df.rename({\n    'start_date': \"Start Date [UTC]\",\n    'end_date': \"End Date [UTC]\",\n    'duration_human': \"Duration\",\n}, axis=\"columns\")\n\n\n\n\n\nCode\ndf[['Start Date [UTC]', 'End Date [UTC]', 'Duration']]\n\n\n\n\nTable 2: Temporal Details of Downloaded CDIP Data\n\n\n\n\n\n\n\n\n\n\nStart Date [UTC]\nEnd Date [UTC]\nDuration\n\n\n\n\n0\n2016-08-26 22:00\n2024-09-11 19:00\n8 years, 17 days\n\n\n1\n2018-08-26 15:00\n2023-07-13 00:00\n4 years, 10 months, 21 days\n\n\n\n\n\n\n\n\n\n\n\nBased the information in Table 2, the CDIP buoy datasets provide extensive coverage for both locations: WETS (CDIP 225) spans approximately 8 years (2016-2024), while Nags Head (CDIP 243) covers nearly 5 years (2018-2023). Both datasets contain three-dimensional displacement measurements at 1.28 Hz sampling frequency."
  },
  {
    "objectID": "index.html#displacement-statistics",
    "href": "index.html#displacement-statistics",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.4 Displacement Statistics",
    "text": "2.4 Displacement Statistics\nFor each location, we analyzed the statistical characteristics of the three-dimensional displacement measurements.\n\n\nCode\nimport duckdb\nimport os\n\ndef calculate_column_stats(partition_path, column_name):\n    con = duckdb.connect()\n    con.execute(\"SET enable_progress_bar = false;\")\n\n    query = f\"\"\"\n    SELECT\n        '{column_name}' as column_name,\n        COUNT({column_name}) as count,\n        COUNT(DISTINCT {column_name}) as unique_count,\n        SUM(CASE WHEN {column_name} IS NULL THEN 1 ELSE 0 END) as null_count,\n        MIN({column_name}) as min_value,\n        MAX({column_name}) as max_value,\n        AVG({column_name}::DOUBLE) as mean,\n        STDDEV({column_name}::DOUBLE) as std_dev,\n        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY {column_name}::DOUBLE) as q1,\n        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY {column_name}::DOUBLE) as median,\n        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY {column_name}::DOUBLE) as q3\n    FROM read_parquet('{partition_path}/**/*.parquet', hive_partitioning=true)\n    WHERE {column_name} IS NOT NULL\n    \"\"\"\n\n    stats_df = con.execute(query).df()\n    con.close()\n    return stats_df\n\ndef analyze_displacement_data(base_path, columns_to_analyze, station_numbers, output_path, overwrite=False):\n    # Check if stats file already exists\n    if os.path.exists(output_path) and not overwrite:\n        return pd.read_parquet(output_path)\n\n    all_stats = []\n\n    for station in station_numbers:\n        station_str = f\"{station:04d}\"  # Format station number with leading zeros\n        partition_path = f\"{base_path}/station_number={station_str}\"\n\n        if not os.path.exists(partition_path):\n            print(f\"Skipping station {station_str} - path does not exist\")\n            continue\n\n        print(f\"Processing station {station_str}...\")\n\n        for column in columns_to_analyze:\n            try:\n                stats_df = calculate_column_stats(partition_path, column)\n                stats_df['station'] = station_str\n                all_stats.append(stats_df)\n                print(f\"  Completed analysis of {column}\")\n            except Exception as e:\n                print(f\"  Error processing {column} for station {station_str}: {str(e)}\")\n\n    # Combine all results\n    if all_stats:\n        combined_stats = pd.concat(all_stats, ignore_index=True)\n\n        # Create output directory if it doesn't exist\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n        # Save to parquet\n        combined_stats.to_parquet(output_path, index=False)\n        print(f\"\\nStatistics saved to {output_path}\")\n        return combined_stats\n    else:\n        print(\"No statistics were generated\")\n        return None\n\n# Example usage\nbase_path = \"../data/a2_std_partition\"\ncolumns_to_analyze = [\n    \"vert_displacement_meters\",\n    \"north_displacement_meters\",\n    \"east_displacement_meters\"\n]\nstation_numbers = [225, 243]  # Add more station numbers as needed\noutput_path = \"../data/displacement_stats.parquet\"\n\n# Run the analysis - will load existing file if it exists\nstats_df = analyze_displacement_data(\n    base_path=base_path,\n    columns_to_analyze=columns_to_analyze,\n    station_numbers=station_numbers,\n    output_path=output_path,\n    overwrite=False  # Set to True to force recalculation\n)\n\nstats_df[\"Range [m]\"] = stats_df['max_value'] + stats_df['min_value'].abs()\n\n# stats_df.loc[stats_df['column_name'] == 'vert_displacement_meters'] = \"Vertical Displacement [m]\"\nstats_df.loc[stats_df['column_name'] == 'vert_displacement_meters', 'column_name'] = \"Vertical Displacement [m]\"\nstats_df.loc[stats_df['column_name'] == 'north_displacement_meters', 'column_name'] = \"North/South Displacement [m]\"\nstats_df.loc[stats_df['column_name'] == 'east_displacement_meters', 'column_name'] = \"East/West Displacement [m]\"\n\nstats_df.loc[stats_df['station'] == '0225', 'station'] = \"225 - Kaneohe Bay, HI\"\nstats_df.loc[stats_df['station'] == '0243', 'station'] = \"243 - Nags Head, NC\"\n# stats_df = stats_df.rename(columns={'vert_displacement_meters': 'Vertical Displacement [m]'})\n\nstats_df = stats_df.rename({\n    \"count\": \"Count\",\n    \"min_value\": \"Min [m]\",\n    \"max_value\": \"Max [m]\",\n    \"mean\": \"Mean [m]\",\n    \"std_dev\": \"Standard Deviation [m]\",\n}, axis=\"columns\")\n\n# stats_df\n\n\n\n\nCode\nimport matplotlib\n\nmatplotlib.rcParams[\"axes.formatter.limits\"] = (-99, 99)\n\n\ndef plot_stat(this_stat_df, stat_column, decimals=2):\n    plt.figure(figsize=(9, 3))\n    plt.gca().yaxis.set_major_formatter(\n        matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), \",\"))\n    )\n    ax = sns.barplot(this_stat_df, x=\"station\", y=stat_column, hue=\"column_name\")\n\n    for container in ax.containers:\n        ax.bar_label(container, fmt=f\"%.{decimals}f\", padding=3, fontsize=7)\n\n    plt.xlabel(None)\n    # Move legend below the plot, set to 3 columns, remove box\n    plt.legend(\n        bbox_to_anchor=(0.5, -0.15), loc=\"upper center\", ncol=3, frameon=False, title=\"\"\n    )\n\n\n\n\n\nCode\nplot_stat(stats_df, 'Count', decimals=0)\n\n\n\n\n\n\n\n\nFigure 8: Available Data: Sample Count\n\n\n\n\n\n\n\nCode\nplot_stat(stats_df, 'Range [m]')\n\n\n\n\n\n\n\n\nFigure 9: Available Data: Range [m]\n\n\n\n\n\n\n\nCode\nplot_stat(stats_df, 'Max [m]')\n\n\n\n\n\n\n\n\nFigure 10: Available Data: Maximum [m]\n\n\n\n\n\n\n\nCode\nplot_stat(stats_df, 'Min [m]')\n\n\n\n\n\n\n\n\nFigure 11: Available Data: Minimum [m]\n\n\n\n\n\n\n\nCode\nplot_stat(stats_df, 'Standard Deviation [m]')\n\n\n\n\n\n\n\n\nFigure 12: Available Data: Standard Deviation [m]\n\n\n\n\n\n\nFigure 9 shows that at WETS (CDIP 225), vertical displacements range approximately \\pm5\\,\\mathrm{m}, while north-south and east-west displacements show similar ranges of about \\pm6\\,\\mathrm{m}. The Nags Head site (CDIP 243) experiences larger displacement ranges, with vertical motion reaching \\pm13\\,\\mathrm{m} and horizontal displacements extending up to \\pm52\\,\\mathrm{m}, reflecting its more dynamic wave climate. All displacement components at both locations show near-zero means with standard deviations between 0.33 and 0.45\\,\\mathrm{m}, indicating symmetric wave motion about the mean position. The anomalous 52\\,\\mathrm{m} range in horizontal displacement at Nags Head suggests potential outliers or measurement artifacts that should be filtered prior to model training to ensure data quality."
  },
  {
    "objectID": "index.html#partitioning-data",
    "href": "index.html#partitioning-data",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.5 Partitioning Data",
    "text": "2.5 Partitioning Data"
  },
  {
    "objectID": "index.html#data-partitioning",
    "href": "index.html#data-partitioning",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.6 Data Partitioning",
    "text": "2.6 Data Partitioning\nTo facilitate efficient data handling and model development, we partitioned the continuous time series into 30-minute segments using a hierarchical storage structure. Each segment contains 2,304 samples (corresponding to the 1.28 Hz sampling rate) and is organized using a hive partitioning strategy based on temporal metadata (year, month, day, hour, minute) and station number.\nThe implementation, shown in Listing 2, creates a systematic file structure where each 30-minute measurement period is stored as an individual parquet file. This organization enables efficient data loading during model training and validation, while maintaining the temporal relationship between segments. The hierarchical structure also facilitates parallel processing and selective data loading based on specific time periods or stations.\n\n\n\n\nListing 2: Data Partitioning Implementation\n\n\ndef partition_df(this_df, station_number, output_folder):\n    # Ensure output folder exists\n    output_folder.mkdir(parents=True, exist_ok=True)\n\n    # Sort the DataFrame by index (assuming timestamp index)\n    this_df = this_df.sort_index()\n\n    # Function to create partition path\n    def create_partition_path(timestamp, station):\n        return (\n            f\"station_number={station:04d}/\"\n            f\"year={timestamp.year:04d}/\"\n            f\"month={timestamp.month:02d}/\"\n            f\"day={timestamp.day:02d}/\"\n            f\"hour={timestamp.hour:02d}/\"\n            f\"minute={timestamp.minute:02d}\"\n        )\n\n    # Process data in chunks of 2304 samples\n    chunk_size = 2304\n    num_chunks = len(this_df) // chunk_size\n\n    for i in range(num_chunks):\n        start_idx = i * chunk_size\n        end_idx = (i + 1) * chunk_size\n\n        # Get chunk of data\n        chunk_df = this_df.iloc[start_idx:end_idx]\n\n        # Verify chunk duration\n        chunk_duration = chunk_df.index[-1] - chunk_df.index[0]\n        expected_duration = timedelta(minutes=30)\n\n        # Use start time of chunk for partitioning\n        chunk_start_time = chunk_df.index[0]\n\n        # Create partition path\n        partition_path = create_partition_path(chunk_start_time, station_number)\n        full_path = output_folder / partition_path\n\n        # Create directory structure\n        full_path.mkdir(parents=True, exist_ok=True)\n\n        # Save the partitioned data\n        output_file = (\n            full_path / f\"data_{chunk_start_time.strftime('%Y%m%d_%H%M')}.parquet\"\n        )\n        chunk_df.to_parquet(output_file)\n\n        print(f\"Saved partition: {partition_path}\")\n        print(f\"Chunk {i} duration: {chunk_duration}\")\n\n    # Handle any remaining data\n    if len(this_df) % chunk_size != 0:\n        remaining_df = this_df.iloc[num_chunks * chunk_size :]\n        print(f\"Warning: {len(remaining_df)} samples remaining at end of file\")\n        print(f\"Last timestamp: {remaining_df.index[-1]}\")"
  },
  {
    "objectID": "index.html#calculating-statistical-wave-parameters",
    "href": "index.html#calculating-statistical-wave-parameters",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.7 Calculating Statistical Wave Parameters",
    "text": "2.7 Calculating Statistical Wave Parameters\nThe original multi year displacement data volume is too large to train a model for this project. As such we need to subset the problem. To do this we will use MHKiT-Python by Fao et al. (2024) to transform the surface elevations into 30 minute statistics of wave measurements. These are easier to visualize and understand the wave characteristics.\nTransforming raw displacement measurements into 30-minute statistical wave parameters using MHKiT-Python Fao et al. (2024) provides a clearer view of the wave conditions at each site. These statistical metrics - significant wave height (H_{m_0}), energy period (T_e), and omnidirectional wave energy flux (J) - help identify unique wave conditions and temporal patterns within the large dataset.\nThe implementation shown in Listing 3 computes these wave parameters.\n\n\n\n\nListing 3: Calculation of Wave Quantities of Interest from Displacement\n\n\nimport mhkit.wave as wave\n\ndef calculate_wave_qoi(input_df, station_number, path):\n    input_df = input_df.dropna(axis=\"index\")\n    column = \"vert_displacement_meters\"\n\n    if len(input_df) != 2304:\n        return None\n\n    sample_rate_hz = 1.28\n    n_fft = 256\n    window = \"hann\"\n    detrend = True\n    surface_elevation = pd.DataFrame(input_df[column].iloc[:2048])\n    spectra = wave.resource.elevation_spectrum(\n        surface_elevation, sample_rate_hz, n_fft, window=window, detrend=detrend\n    )\n\n    return {\n        \"time\": input_df.index[0],\n        \"significant_wave_height_meters\": wave.resource.significant_wave_height(spectra)[\"Hm0\"].to_list()[0],  # type: ignore\n        \"energy_period_seconds\": wave.resource.energy_period(spectra)[\"Te\"].to_list()[0],  # type: ignore\n        \"omnidirectional_wave_energy_flux\": wave.resource.energy_flux(spectra, np.nan, deep=True)[\"J\"].to_list()[0],  # type: ignore\n        \"station_number\": station_number,\n        \"path\": str(path),\n    }\n\n\n\n\n\n\nCode\nqoi_225 = pd.read_parquet(\"../data/b2_wave_qoi_stats/qoi_225.parquet\")\nqoi_243 = pd.read_parquet(\"../data/b2_wave_qoi_stats/qoi_243.parquet\")\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme()\n\n\ndef compare_qoi_over_time(qoi, label):\n    figsize = (8, 3)\n    xlabel = \"Time\"\n    qoi_225[qoi].plot(\n        figsize=figsize, xlabel=xlabel, ylabel=label, linewidth=0.75\n    )\n    plt.show()\n\n    qoi_243[qoi].plot(\n        figsize=figsize, xlabel=xlabel, ylabel=label, linewidth=0.75\n    )\n    plt.show()\n\ncompare_qoi_over_time('significant_wave_height_meters', \"$H_{m_0}$ [$m$]\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Wave Energy Test Site - Hawaii\n\n\n\n\n\n\n\n\n\n\n\n(b) Nags Head - North Carolina\n\n\n\n\n\n\n\nFigure 13: Significant Wave Height, H_{m_0} [m]\n\n\n\n\nCode\ncompare_qoi_over_time('energy_period_seconds', \"$T_e$ [$s$]\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Wave Energy Test Site - Hawaii\n\n\n\n\n\n\n\n\n\n\n\n(b) Nags Head - North Carolina\n\n\n\n\n\n\n\nFigure 14: Energy Period, T_e [s]\n\n\n\n\nCode\ncompare_qoi_over_time('omnidirectional_wave_energy_flux', \"$J$ [$W/m$]\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Wave Energy Test Site - Hawaii\n\n\n\n\n\n\n\n\n\n\n\n(b) Nags Head - North Carolina\n\n\n\n\n\n\n\nFigure 15: Omnidirectional Wave Energy Flux (Wave Power), J [W/m]\n\n\n\n\n\n\nCode\nstart_date = \"2019-01-01 00:00:00\"\nend_date = \"2019-12-31 23:59:59.9999\"\n\nqoi_225 = qoi_225.loc[start_date:end_date]\nqoi_243 = qoi_243.loc[start_date:end_date]\n\n\ncompare_qoi_over_time('significant_wave_height_meters', \"$H_{m_0}$ [$m$]\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Wave Energy Test Site - Hawaii\n\n\n\n\n\n\n\n\n\n\n\n(b) Nags Head - North Carolina\n\n\n\n\n\n\n\nFigure 16: Significant Wave Height, H_{m_0} [m]\n\n\n\n\nCode\ncompare_qoi_over_time('energy_period_seconds', \"$T_e$ [$s$]\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Wave Energy Test Site - Hawaii\n\n\n\n\n\n\n\n\n\n\n\n(b) Nags Head - North Carolina\n\n\n\n\n\n\n\nFigure 17: Energy Period, T_e [s]\n\n\n\n\nCode\ncompare_qoi_over_time('omnidirectional_wave_energy_flux', \"$J$ [$W/m$]\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Wave Energy Test Site - Hawaii\n\n\n\n\n\n\n\n\n\n\n\n(b) Nags Head - North Carolina\n\n\n\n\n\n\n\nFigure 18: Omnidirectional Wave Energy Flux (Wave Power), J [W/m]\n\n\n\n\nAs shown in Figure 13 through Figure 15, WETS (CDIP 225) exhibits more periodic behavior while Nags Head (CDIP 243) shows greater variability. A notable data gap exists in the WETS measurements from 2021 to 2023. Focusing on 2019 (Figure 16 through Figure 18) highlights that Nags Head experiences higher peak wave power and wave heights but a narrower range of wave periods compared to WETS. These insights will guide our selection of representative data segments for model development."
  },
  {
    "objectID": "index.html#sea-state-analysis",
    "href": "index.html#sea-state-analysis",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.8 Sea State Analysis",
    "text": "2.8 Sea State Analysis\nTo understand the distribution of wave conditions at each site, we developed sea state matrices that bin the data by significant wave height (H_{m_0}) and energy period (T_e). This categorization quantifies the frequency of different wave conditions and helps ensure our training dataset encompasses a representative range of height and period states.\n\n\nCode\nimport numpy as np\n\ndef plot_wave_heatmap(df, figsize=(12, 8)):\n    # Create bins for Hm0 and Te\n    hm0_bins = np.arange(0, df['significant_wave_height_meters'].max() + 0.5, 0.5)\n    te_bins = np.arange(0, df['energy_period_seconds'].max() + 1, 1)\n\n    # Use pd.cut to bin the data\n    hm0_binned = pd.cut(df['significant_wave_height_meters'],\n                        bins=hm0_bins,\n                        labels=hm0_bins[:-1],\n                        include_lowest=True)\n\n    te_binned = pd.cut(df['energy_period_seconds'],\n                       bins=te_bins,\n                       labels=te_bins[:-1],\n                       include_lowest=True)\n\n    # Create cross-tabulation of binned data\n    counts = pd.crosstab(hm0_binned, te_binned)\n\n    counts = counts.sort_index(ascending=False)\n\n\n    # Replace 0 counts with NaN\n    counts = counts.replace(0, np.nan)\n\n    # Create figure and axis\n    plt.figure(figsize=figsize)\n\n    # Create heatmap using seaborn\n    ax = sns.heatmap(\n        counts,\n        cmap='viridis',\n        annot=True,  # Add count annotations\n        fmt='.0f',   # Format annotations as integers\n        cbar_kws={'label': 'Count'},\n    )\n\n    # Customize plot\n    plt.xlabel('Energy Period Te (s)')\n    plt.ylabel('Significant Wave Height Hm0 (m)')\n\n    # Rotate x-axis labels for better readability\n    # plt.xticks(rotation=45)\n    plt.yticks(rotation=90)\n\n    # Adjust layout to prevent label cutoff\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nCode\nqoi_225 = pd.read_parquet(\"../data/b2_wave_qoi_stats/qoi_225.parquet\")\nqoi_243 = pd.read_parquet(\"../data/b2_wave_qoi_stats/qoi_243.parquet\")\n\n\n\n\n\nCode\nplot_wave_heatmap(qoi_225)\n\n\n\n\n\n\n\n\nFigure 19: Distribution of Sea States at WETS (CDIP 225), Showing Occurrence Count of Combined Significant Wave Height (H_{m_0}) and Energy Period (T_e) Conditions\n\n\n\n\n\n\n\nCode\nplot_wave_heatmap(qoi_243)\n\n\n\n\n\n\n\n\nFigure 20: Distribution of Sea States at Nags Head (CDIP 243), Showing Occurrence Count of Combined Significant Wave Height (H_{m_0}) and Energy Period (T_e) Conditions\n\n\n\n\n\n\nThe sea state matrices reveal distinct wave climates at each location. As shown in Figure 19, the WETS site exhibits wave heights predominantly below 5 m, with energy periods clustered between 6-12 seconds. In contrast, Figure 20 shows that Nags Head experiences a broader range of conditions, with wave heights reaching 7.5 m and energy periods spanning 3-15 seconds. The presence of 3-second periods at Nags Head likely indicates local wind-generated waves, a characteristic absent at WETS.\nThese differing wave climate characteristics confirm that our dataset captures a diverse range of sea states across both locations, providing a robust foundation for model development. The comprehensive coverage of wave conditions suggests we can proceed with creating training datasets that will expose our models to the full spectrum of expected wave behaviors."
  },
  {
    "objectID": "index.html#structured-sampling-method",
    "href": "index.html#structured-sampling-method",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "3.1 Structured Sampling Method",
    "text": "3.1 Structured Sampling Method\nBased on the sea state distributions shown in Figure 19 and Figure 20, we developed a systematic sampling method to create balanced training datasets. The approach, implemented in Listing 4, samples time series segments from each combination of significant wave height and energy period bins, ensuring representation across the full range of observed wave conditions.\n\n\n\n\n\nListing 4: Function to Sample Sea State Matrix Bins\n\n\ndef sample_wave_bins(df, n_samples=1, hm0_step=0.5, te_step=1.0):\n    # Create bins for Hm0 and Te\n    hm0_bins = np.arange(0, df['significant_wave_height_meters'].max() + hm0_step, hm0_step)\n    te_bins = np.arange(0, df['energy_period_seconds'].max() + te_step, te_step)\n\n    # Add bin columns to the dataframe\n    df_binned = df.copy()\n    df_binned['hm0_bin'] = pd.cut(df['significant_wave_height_meters'],\n                                 bins=hm0_bins,\n                                 labels=hm0_bins[:-1],\n                                 include_lowest=True)\n\n    df_binned['te_bin'] = pd.cut(df['energy_period_seconds'],\n                                bins=te_bins,\n                                labels=te_bins[:-1],\n                                include_lowest=True)\n\n    # Convert category types to float\n    df_binned['hm0_bin'] = df_binned['hm0_bin'].astype(float)\n    df_binned['te_bin'] = df_binned['te_bin'].astype(float)\n\n    # Sample from each bin combination\n    samples = []\n    for hm0_val in df_binned['hm0_bin'].unique():\n        for te_val in df_binned['te_bin'].unique():\n            bin_data = df_binned[\n                (df_binned['hm0_bin'] == hm0_val) &\n                (df_binned['te_bin'] == te_val)\n            ]\n\n            if not bin_data.empty:\n                # Sample min(n_samples, bin size) rows from this bin\n                bin_samples = bin_data.sample(\n                    n=min(n_samples, len(bin_data)),\n                    random_state=42  # For reproducibility\n                )\n                samples.append(bin_samples)\n\n    # Combine all samples\n    if samples:\n        result = pd.concat(samples, axis=0).reset_index(drop=True)\n\n        # Add bin center values for reference\n        result['hm0_bin_center'] = result['hm0_bin'] + (hm0_step / 2)\n        result['te_bin_center'] = result['te_bin'] + (te_step / 2)\n        result.insert(0, 'station_number', result.pop('station_number'))\n\n\n        return result\n    else:\n        return pd.DataFrame()"
  },
  {
    "objectID": "index.html#training-dataset-creation",
    "href": "index.html#training-dataset-creation",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "3.2 Training Dataset Creation",
    "text": "3.2 Training Dataset Creation\n\n\nCode\ndata_225 = sample_wave_bins(qoi_225)\ndata_225.to_parquet(\"../model_input_spec_225.parquet\")\n\n\n\n\nCode\ndata_225.head()\n\n\n\n\n\n\n\n\n\nstation_number\nsignificant_wave_height_meters\nenergy_period_seconds\nomnidirectional_wave_energy_flux\npath\nhm0_bin\nte_bin\nhm0_bin_center\nte_bin_center\n\n\n\n\n0\n225\n1.116275\n5.640104\n3445.602238\n/Users/asmacbook/Desktop/Programming/msds/dtsa...\n1.0\n5.0\n1.25\n5.5\n\n\n1\n225\n1.352471\n6.033005\n5410.344970\n/Users/asmacbook/Desktop/Programming/msds/dtsa...\n1.0\n6.0\n1.25\n6.5\n\n\n2\n225\n1.087143\n7.104669\n4116.732805\n/Users/asmacbook/Desktop/Programming/msds/dtsa...\n1.0\n7.0\n1.25\n7.5\n\n\n3\n225\n1.068228\n8.150482\n4559.810465\n/Users/asmacbook/Desktop/Programming/msds/dtsa...\n1.0\n8.0\n1.25\n8.5\n\n\n4\n225\n1.070689\n9.320074\n5238.196702\n/Users/asmacbook/Desktop/Programming/msds/dtsa...\n1.0\n9.0\n1.25\n9.5\n\n\n\n\n\n\n\n\n\nCode\ndata_225.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 76 entries, 0 to 75\nData columns (total 9 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   station_number                    76 non-null     object \n 1   significant_wave_height_meters    76 non-null     float64\n 2   energy_period_seconds             76 non-null     float64\n 3   omnidirectional_wave_energy_flux  76 non-null     float64\n 4   path                              76 non-null     object \n 5   hm0_bin                           76 non-null     float64\n 6   te_bin                            76 non-null     float64\n 7   hm0_bin_center                    76 non-null     float64\n 8   te_bin_center                     76 non-null     float64\ndtypes: float64(7), object(2)\nmemory usage: 5.5+ KB\n\n\n\n\nCode\ndata_243 = sample_wave_bins(qoi_243)\ndata_243.to_parquet(\"../model_input_spec_243.parquet\")\n\n\n\n\nCode\ndata_243.head()\n\n\n\n\n\n\n\n\n\nstation_number\nsignificant_wave_height_meters\nenergy_period_seconds\nomnidirectional_wave_energy_flux\npath\nhm0_bin\nte_bin\nhm0_bin_center\nte_bin_center\n\n\n\n\n0\n243\n0.969098\n6.869854\n3163.139256\n/Users/asmacbook/Desktop/Programming/msds/dtsa...\n0.5\n6.0\n0.75\n6.5\n\n\n1\n243\n0.751133\n5.046030\n1395.787017\n/Users/asmacbook/Desktop/Programming/msds/dtsa...\n0.5\n5.0\n0.75\n5.5\n\n\n2\n243\n0.736608\n4.954708\n1318.034155\n/Users/asmacbook/Desktop/Programming/msds/dtsa...\n0.5\n4.0\n0.75\n4.5\n\n\n3\n243\n0.877664\n7.485963\n2827.089576\n/Users/asmacbook/Desktop/Programming/msds/dtsa...\n0.5\n7.0\n0.75\n7.5\n\n\n4\n243\n0.676855\n8.722353\n1959.116238\n/Users/asmacbook/Desktop/Programming/msds/dtsa...\n0.5\n8.0\n0.75\n8.5\n\n\n\n\n\n\n\n\n\nCode\ndata_243.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 112 entries, 0 to 111\nData columns (total 9 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   station_number                    112 non-null    object \n 1   significant_wave_height_meters    112 non-null    float64\n 2   energy_period_seconds             112 non-null    float64\n 3   omnidirectional_wave_energy_flux  112 non-null    float64\n 4   path                              112 non-null    object \n 5   hm0_bin                           112 non-null    float64\n 6   te_bin                            112 non-null    float64\n 7   hm0_bin_center                    112 non-null    float64\n 8   te_bin_center                     112 non-null    float64\ndtypes: float64(7), object(2)\nmemory usage: 8.0+ KB\n\n\nInitial sampling with one sample per bin yielded 76 half-hour segments for WETS (CDIP 225) and 112 segments for Nags Head (CDIP 243), providing a manageable dataset for initial model development. To explore the impact of dataset size on model performance, we also created expanded datasets with five samples per bin, resulting in 380 segments for WETS and 560 segments for Nags Head. While these larger datasets offer more comprehensive coverage of wave conditions, they require significantly more computational resources for training.\nThis sampling approach ensures our training data captures the diverse wave conditions present at each site while maintaining computational feasibility. The structured nature of the sampling helps prevent bias toward more common wave conditions, potentially improving model robustness across different sea states.\n\n\nCode\nn_samples = 5\ndata_225 = sample_wave_bins(qoi_225, n_samples=n_samples)\ndata_225.to_parquet(f\"../model_input.samples_{n_samples}._spec_225.parquet\")\n\ndata_243 = sample_wave_bins(qoi_243, n_samples=n_samples)\ndata_243.to_parquet(f\"../model_input.samples_{n_samples}.spec_243.parquet\")\n\n\n\n\nCode\nprint(len(data_225))\n\n\n353\n\n\n\n\nCode\nprint(len(data_243))\n\n\n493\n\n\nAdditional datasets with five samples per bin were also created (353 segments for WETS and 493 for Nags Head) and archived for future research, though this project focuses on the more computationally manageable single-sample datasets."
  },
  {
    "objectID": "index.html#lstm-model",
    "href": "index.html#lstm-model",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "4.1 LSTM Model",
    "text": "4.1 LSTM Model\nOur base LSTM model, shown in Listing 5, provides a straightforward approach to sequence prediction. The model processes three-dimensional displacement inputs through stacked LSTM layers with dropout regularization. This architecture enables the model to learn wave patterns at different time scales. The final linear layer maps the LSTM outputs back to displacement predictions, creating a direct sequence-to-sequence prediction framework.\n\n\n\n\nListing 5: Long Short-Term Memory PyTorch Model\n\n\nclass LSTMModel(WavePredictionModel):\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int = 128,\n        num_layers: int = 2,\n        dropout: float = 0.2,\n        learning_rate: float = 1e-3,\n    ):\n        super().__init__(input_dim, learning_rate)\n        self.save_hyperparameters()  # Save all init parameters\n\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n\n        self.lstm = nn.LSTM(\n            input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout\n        )\n        self.fc = nn.Linear(hidden_dim, input_dim)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        lstm_out, _ = self.lstm(x)\n        predictions = self.fc(lstm_out)\n        return predictions"
  },
  {
    "objectID": "index.html#enhanced-lstm-model",
    "href": "index.html#enhanced-lstm-model",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "4.2 Enhanced LSTM Model",
    "text": "4.2 Enhanced LSTM Model\nThe enhanced LSTM implementation, detailed in Listing 6, extends the base model with several architectural improvements. Bidirectional processing allows the model to consider both past and future context when making predictions. The addition of skip connections helps maintain gradient flow through the deep network, while layer normalization stabilizes training.\n\n\n\n\nListing 6: Enhanced Long Short-Term Memory PyTorch Model\n\n\nclass EnhancedLSTMModel(WavePredictionModel):\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int = 128,\n        num_layers: int = 2,\n        dropout: float = 0.2,\n        learning_rate: float = 1e-3,\n        bidirectional: bool = True,\n    ):\n        super().__init__(input_dim, learning_rate)\n        self.save_hyperparameters()\n\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n\n        # Input processing\n        self.input_layer = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout / 2),\n        )\n\n        # Main LSTM layers with skip connections\n        self.lstm_layers = nn.ModuleList()\n        self.layer_norms = nn.ModuleList()\n\n        lstm_input_dim = hidden_dim\n        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n\n        for _ in range(num_layers):\n            self.lstm_layers.append(\n                nn.LSTM(\n                    lstm_input_dim,\n                    hidden_dim,\n                    num_layers=1,\n                    batch_first=True,\n                    bidirectional=bidirectional,\n                    dropout=0,\n                )\n            )\n            self.layer_norms.append(nn.LayerNorm(lstm_output_dim))\n            lstm_input_dim = lstm_output_dim\n\n        # Output processing\n        self.output_layers = nn.ModuleList(\n            [\n                nn.Linear(lstm_output_dim, hidden_dim),\n                nn.Linear(hidden_dim, hidden_dim // 2),\n                nn.Linear(hidden_dim // 2, input_dim),\n            ]\n        )\n\n        self.dropouts = nn.ModuleList(\n            [nn.Dropout(dropout) for _ in range(len(self.output_layers))]\n        )\n\n        # Skip connection\n        self.skip_connection = nn.Linear(input_dim, input_dim)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Store original input for skip connection\n        original_input = x\n\n        # Input processing\n        x = self.input_layer(x)\n\n        # Process through LSTM layers with residual connections\n        for lstm, norm in zip(self.lstm_layers, self.layer_norms):\n            residual = x\n            x, _ = lstm(x)\n            x = norm(x)\n            if residual.shape == x.shape:\n                x = x + residual\n\n        # Output processing\n        for linear, dropout in zip(self.output_layers[:-1], self.dropouts[:-1]):\n            residual = x\n            x = linear(x)\n            x = F.relu(x)\n            x = dropout(x)\n            if residual.shape == x.shape:\n                x = x + residual\n\n        # Final output layer\n        x = self.output_layers[-1](x)\n\n        # Add skip connection\n        x = x + self.skip_connection(original_input)\n\n        return x\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)"
  },
  {
    "objectID": "index.html#transformer-model",
    "href": "index.html#transformer-model",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "4.3 Transformer Model",
    "text": "4.3 Transformer Model\nOur Transformer implementation, shown in Listing 7, takes a fundamentally different approach to sequence modeling. Rather than processing the wave motion sequentially, the model uses self-attention mechanisms to directly capture relationships between any two points in the input sequence. This architecture, combined with multi-head attention and position-wise feed-forward networks, should enable the model to identify both short-term wave patterns and longer-range dependencies in the displacement data.\n\n\n\n\nListing 7: Transformer PyTorch Model\n\n\nclass TransformerModel(WavePredictionModel):\n\n    def __init__(\n        self,\n        input_dim: int,\n        d_model: int = 128,\n        nhead: int = 8,\n        num_layers: int = 4,\n        dropout: float = 0.2,\n        learning_rate: float = 1e-3,\n    ):\n        super().__init__(input_dim, learning_rate)\n        self.save_hyperparameters()  # Save all init parameters\n\n        self.input_projection = nn.Linear(input_dim, d_model)\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=d_model * 4,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layer, num_layers=num_layers\n        )\n\n        self.output_projection = nn.Linear(d_model, input_dim)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.input_projection(x)\n        x = self.transformer_encoder(x)\n        return self.output_projection(x)"
  },
  {
    "objectID": "index.html#training-metrics",
    "href": "index.html#training-metrics",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "5.1 Training Metrics",
    "text": "5.1 Training Metrics\n\n5.1.1 Baseline Model\n\n\n\nCode\ndf = pd.read_parquet(\"../training_history/training_history_lstm_20241207_205254.parquet\")\ndf = df.set_index(['epoch'])\n\nfig, axs = plt.subplots(figsize=(12, 2))\ndf[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel=\"Epoch\", ylabel=\"Mean Absolute Error\")\naxs.legend(['Train', \"Validate\"])\nplt.show()\n\ndf = pd.read_parquet(\"../training_history/training_history_lstm_20241207_215544.parquet\")\ndf = df.set_index(['epoch'])\n\nfig, axs = plt.subplots(figsize=(12, 2))\ndf[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel=\"Epoch\", ylabel=\"Mean Absolute Error\")\naxs.legend(['Train', \"Validate\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) CDIP 225\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) CDIP 243\n\n\n\n\n\n\n\nFigure 21: Baseline Model: Training vs. Validation Mean Absolute Error\n\n\n\n\n\n\n\n5.1.2 100 Epoch LSTM Model\n\n\n\nCode\ndf = pd.read_parquet(\"../training_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_100/\")\ndf = df.set_index(['epoch'])\n\nfig, axs = plt.subplots(figsize=(12, 2))\ndf[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel=\"Epoch\", ylabel=\"Mean Absolute Error\")\naxs.legend(['Train', \"Validate\"])\nplt.show()\n\ndf = pd.read_parquet(\"../training_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_100/\")\ndf = df.set_index(['epoch'])\n\nfig, axs = plt.subplots(figsize=(12, 2))\ndf[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel=\"Epoch\", ylabel=\"Mean Absolute Error\")\naxs.legend(['Train', \"Validate\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) CDIP 225\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) CDIP 243\n\n\n\n\n\n\n\nFigure 22: 100 Epoch LSTM Model: Training vs. Validation Mean Absolute Error\n\n\n\n\n\n\n\n5.1.3 Transformer Model\n\n\n\nCode\ndf = pd.read_parquet(\"../training_history/model_transformer.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/\")\ndf = df.set_index(['epoch'])\n\nfig, axs = plt.subplots(figsize=(12, 2))\ndf[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel=\"Epoch\", ylabel=\"Mean Absolute Error\")\naxs.legend(['Train', \"Validate\"])\n\nplt.show()\n\ndf = pd.read_parquet(\"../training_history/model_transformer.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/\")\ndf = df.set_index(['epoch'])\nfig, axs = plt.subplots(figsize=(12, 2))\ndf[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel=\"Epoch\", ylabel=\"Mean Absolute Error\")\naxs.legend(['Train', \"Validate\"])\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) CDIP 225\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) CDIP 243\n\n\n\n\n\n\n\nFigure 23: Transformer Model: Training vs. Validation Mean Absolute Error\n\n\n\n\n\n\n\n5.1.4 4 Layer LSTM Model\n\n\n\nCode\ndf = pd.read_parquet(\"../training_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_4.EPOCHS_25/\")\ndf = df.set_index(['epoch'])\n\nfig, axs = plt.subplots(figsize=(12, 2))\ndf[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel=\"Epoch\", ylabel=\"Mean Absolute Error\")\naxs.legend(['Train', \"Validate\"])\nplt.show()\n\ndf = pd.read_parquet(\"../training_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_4.EPOCHS_25/\")\ndf = df.set_index(['epoch'])\n\nfig, axs = plt.subplots(figsize=(12, 2))\ndf[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel=\"Epoch\", ylabel=\"Mean Absolute Error\")\naxs.legend(['Train', \"Validate\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) CDIP 225\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) CDIP 243\n\n\n\n\n\n\n\nFigure 24: 4 Layer LSTM Model: Training vs. Validation Mean Absolute Error\n\n\n\n\n\n\n\n5.1.5 6 Layer LSTM Model\n\n\n\nCode\ndf = pd.read_parquet(\"../training_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_6.EPOCHS_25/\")\ndf = df.set_index(['epoch'])\n\nfig, axs = plt.subplots(figsize=(12, 2))\ndf[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel=\"Epoch\", ylabel=\"Mean Absolute Error\")\naxs.legend(['Train', \"Validate\"])\n\nplt.show()\n\ndf = pd.read_parquet(\"../training_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_6.EPOCHS_25/\")\ndf = df.set_index(['epoch'])\n\nfig, axs = plt.subplots(figsize=(12, 2))\ndf[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel=\"Epoch\", ylabel=\"Mean Absolute Error\")\naxs.legend(['Train', \"Validate\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) CDIP 225\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) CDIP 243\n\n\n\n\n\n\n\nFigure 25: 6 Layer LSTM Model: Training vs. Validation Mean Absolute Error\n\n\n\n\n\n\n\n5.1.6 Enhanced Transformer Model\n\n\n\nCode\ndf = pd.read_parquet(\"../training_history/model_enhanced_transformer.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/\")\ndf = df.set_index(['epoch'])\n\nfig, axs = plt.subplots(figsize=(12, 2))\ndf[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel=\"Epoch\", ylabel=\"Mean Absolute Error\")\naxs.legend(['Train', \"Validate\"])\n\nplt.show()\n\ndf = pd.read_parquet(\"../training_history/model_enhanced_transformer.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/\")\ndf = df.set_index(['epoch'])\n\nfig, axs = plt.subplots(figsize=(12, 2))\ndf[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel=\"Epoch\", ylabel=\"Mean Absolute Error\")\naxs.legend(['Train', \"Validate\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) CDIP 225\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) CDIP 243\n\n\n\n\n\n\n\nFigure 26: Enhanced Transformer Model: Training vs. Validation Mean Absolute Error\n\n\n\n\n\n\n\n5.1.7 Enhanced LSTM Model\n\n\n\nCode\ndf = pd.read_parquet(\"../training_history/model_enhanced_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/\")\ndf = df.set_index(['epoch'])\n\nfig, axs = plt.subplots(figsize=(12, 2))\ndf[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel=\"Epoch\", ylabel=\"Mean Absolute Error\")\naxs.legend(['Train', \"Validate\"])\n\nplt.show()\n\ndf = pd.read_parquet(\"../training_history/model_enhanced_transformer.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/\")\ndf = df.set_index(['epoch'])\n\nfig, axs = plt.subplots(figsize=(12, 2))\ndf[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel=\"Epoch\", ylabel=\"Mean Absolute Error\")\naxs.legend(['Train', \"Validate\"])\n\n\n\n\n\n\n\n\n\n\n\n\n(a) CDIP 225\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) CDIP 243\n\n\n\n\n\n\n\nFigure 27: Enhanced LSTM Model: Training vs. Validation Mean Absolute Error"
  },
  {
    "objectID": "index.html#training-results-summary",
    "href": "index.html#training-results-summary",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "5.2 Training Results Summary",
    "text": "5.2 Training Results Summary\nThe training results, shown in Figure 21 through Figure 27, indicate superior performance from the LSTM-based models. All LSTM variants demonstrated stable learning curves without significant overfitting. Notably, the 100-epoch LSTM model showed continued improvement in both training and validation loss, suggesting potential benefits from extended training periods. The Transformer models, while marginally functional, generally showed poor learning patterns compared to their LSTM counterparts."
  },
  {
    "objectID": "index.html#select-visual-analysis",
    "href": "index.html#select-visual-analysis",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "6.1 Select Visual Analysis",
    "text": "6.1 Select Visual Analysis\nTo examine model performance across different wave conditions, we present a series of representative time series comparisons. Each figure shows a 128-sample window of predictions alongside the corresponding measured wave displacements, with varying combinations of significant wave height (H_{m_0}) and energy period (T_e). These comparisons provide insight into how the models handle different wave states and reveal characteristic prediction patterns.\n\n6.1.1 Baseline LSTM Timeseries\n\n\n6.1.2 CDIP 225 - Kaneohe Bay, HI\n\n\n\nCode\ndf_bins = pd.read_parquet(\"./model_input_spec_225.parquet\")\ntargets = pd.read_parquet(\"../testing_history/model_lstm.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.targets.lstm_20241207_205411.parquet\")\npredictions = pd.read_parquet(\"../testing_history/model_lstm.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.predictions.lstm_20241207_205411.parquet\")\nsources = pd.read_parquet(\"../testing_history/model_lstm.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.sources.lstm_20241207_205411.parquet\")\n\nresult_stats.append(calc_stats(\"Baseline\", \"225\", targets, predictions))\n\n\n\n\nCode\nplot_test_section(0, df_bins, sources, targets, predictions)\n\n\n\n\n\n\n\n\nFigure 28: Baseline LSTM - CDIP 225 - Test Section 1\n\n\n\n\n\n\n\nCode\nplot_test_section(1, df_bins, sources, targets, predictions)\n\n\n\n\n\n\n\n\nFigure 29: Baseline LSTM - CDIP 225 - Test Section 2\n\n\n\n\n\n\n\nCode\nplot_test_section(2, df_bins, sources, targets, predictions)\n\n\n\n\n\n\n\n\nFigure 30: Baseline LSTM - CDIP 225 - Test Section 3\n\n\n\n\n\n\n\nCode\nplot_test_section(3, df_bins, sources, targets, predictions)\n\n\n\n\n\n\n\n\nFigure 31: Baseline LSTM - CDIP 225 - Test Section 4\n\n\n\n\n\n\n\nCode\nplot_test_section(4, df_bins, sources, targets, predictions)\n\n\n\n\n\n\n\n\nFigure 32: Baseline LSTM - CDIP 225 - Test Section 5\n\n\n\n\n\n\n\nCode\nplot_test_section(5, df_bins, sources, targets, predictions)\n\n\n\n\n\n\n\n\nFigure 33: Baseline LSTM - CDIP 225 - Test Section 6\n\n\n\n\n\n\n\n\n6.1.3 CDIP 243 - Nags Head, NC\n\n\n\nCode\ntargets = pd.read_parquet(\"../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.targets.lstm_20241207_215743.parquet\")\npredictions = pd.read_parquet(\"../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.predictions.lstm_20241207_215743.parquet\")\nsources = pd.read_parquet(\"../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.sources.lstm_20241207_215743.parquet\")\ndf_bins = pd.read_parquet(\"./model_input_spec_243.parquet\")\n\nresult_stats.append(calc_stats(\"Baseline\", \"243\", targets, predictions))\n\n\n\n\nCode\nplot_test_section(0, df_bins, sources, targets, predictions)\n\n\n\n\n\n\n\n\nFigure 34: Baseline LSTM - CDIP 243 - Test Section 1\n\n\n\n\n\n\n\nCode\nplot_test_section(1, df_bins, sources, targets, predictions)\n\n\n\n\n\n\n\n\nFigure 35: Baseline LSTM - CDIP 243 - Test Section 2\n\n\n\n\n\n\n\nCode\nplot_test_section(2, df_bins, sources, targets, predictions)\n\n\n\n\n\n\n\n\nFigure 36: Baseline LSTM - CDIP 243 - Test Section 3\n\n\n\n\n\n\n\nCode\nplot_test_section(3, df_bins, sources, targets, predictions)\n\n\n\n\n\n\n\n\nFigure 37: Baseline LSTM - CDIP 243 - Test Section 4\n\n\n\n\n\n\n\nCode\nplot_test_section(4, df_bins, sources, targets, predictions)\n\n\n\n\n\n\n\n\nFigure 38: Baseline LSTM - CDIP 243 - Test Section 5\n\n\n\n\n\n\n\nCode\nplot_test_section(5, df_bins, sources, targets, predictions)\n\n\n\n\n\n\n\n\nFigure 39: Baseline LSTM - CDIP 243 - Test Section 6\n\n\n\n\n\n\nThe time series comparisons, shown in Figure 28 through Figure 39, reveal varying prediction quality across different wave conditions. A common pattern emerges where predictions show larger errors at the start of each sequence but improve towards the end. This behavior suggests the models require several time steps to establish the wave state context before making accurate predictions. The LSTM models in particular demonstrate this adaptation, likely due to their ability to build and refine internal state representations as they process the sequence. Overall, the baseline model demonstrates capacity to capture underlying wave patterns, though prediction accuracy varies with wave state conditions."
  },
  {
    "objectID": "index.html#quantitive-visualization",
    "href": "index.html#quantitive-visualization",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "6.2 Quantitive Visualization",
    "text": "6.2 Quantitive Visualization\n\n6.2.1 Baseline Model\n\n\n\nCode\ntargets = pd.read_parquet(\"../testing_history/model_lstm.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.targets.lstm_20241207_205411.parquet\")\npredictions = pd.read_parquet(\"../testing_history/model_lstm.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.predictions.lstm_20241207_205411.parquet\")\nsources = pd.read_parquet(\"../testing_history/model_lstm.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.sources.lstm_20241207_205411.parquet\")\ndf_bins = pd.read_parquet(\"./model_input_spec_225.parquet\")\n\ntargets *= 0.3\npredictions *= 0.3\n\n\ntargets['vert_displacement_meters'].plot(figsize=(16, 4), label=\"target\", linewidth = 0.85)\npredictions['vert_displacement_meters'].plot(label=\"prediction\", linewidth=0.75)\nplt.ylabel(\"Vertical Displacement [m]\")\nplt.legend(labels=[\"Train\", \"Test\"])\nplt.show()\n\ntargets = pd.read_parquet(\"../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.targets.lstm_20241207_215743.parquet\")\npredictions = pd.read_parquet(\"../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.predictions.lstm_20241207_215743.parquet\")\nsources = pd.read_parquet(\"../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.sources.lstm_20241207_215743.parquet\")\ndf_bins = pd.read_parquet(\"./model_input_spec_243.parquet\")\n\ntargets *= 0.3\npredictions *= 0.3\n\n\ntargets['vert_displacement_meters'].plot(figsize=(16, 4), label=\"target\", linewidth = 0.85)\npredictions['vert_displacement_meters'].plot(label=\"prediction\", linewidth=0.75)\nplt.ylabel(\"Vertical Displacement [m]\")\nplt.legend(labels=[\"Train\", \"Test\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) CDIP 225\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) CDIP 243\n\n\n\n\n\n\n\nFigure 40: Baseline LSTM - All Test Sections\n\n\n\n\n\n\n\n6.2.2 Transformer Model\n\n\n\nCode\ntargets = pd.read_parquet(\"../testing_history/model_transformer.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.targets.transformer_20241207_231033.parquet\")\npredictions = pd.read_parquet(\"../testing_history/model_transformer.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.predictions.transformer_20241207_231033.parquet\")\nsources = pd.read_parquet(\"../testing_history/model_transformer.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.sources.transformer_20241207_231033.parquet\")\n\ndf_bins = pd.read_parquet(\"./model_input_spec_225.parquet\")\n\n\ntargets['vert_displacement_meters'].plot(figsize=(16, 4), label=\"target\", linewidth = 0.85)\npredictions['vert_displacement_meters'].plot(label=\"prediction\", linewidth=0.75)\nplt.ylabel(\"Vertical Displacement [m]\")\nplt.show()\nresult_stats.append(calc_stats(\"Transformer\", \"225\", targets, predictions))\n\ntargets = pd.read_parquet(\"../testing_history/model_transformer.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.targets.transformer_20241207_235408.parquet\")\npredictions = pd.read_parquet(\"../testing_history/model_transformer.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.predictions.transformer_20241207_235408.parquet\")\nsources = pd.read_parquet(\"../testing_history/model_transformer.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.sources.transformer_20241207_235408.parquet\")\n\ndf_bins = pd.read_parquet(\"./model_input_spec_243.parquet\")\n\n\ntargets['vert_displacement_meters'].plot(figsize=(16, 4), label=\"target\", linewidth = 0.85)\npredictions['vert_displacement_meters'].plot(label=\"prediction\", linewidth=0.75)\nplt.ylabel(\"Vertical Displacement [m]\")\nplt.show()\n\nresult_stats.append(calc_stats(\"Transformer\", \"243\", targets, predictions))\n\n\n\n\n\n\n\n\n\n\n\n\n(a) CDIP 225\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) CDIP 243\n\n\n\n\n\n\n\nFigure 41: Transformer - All Test Sections\n\n\n\n\n\n\n\n6.2.3 LSTM 100 Epoch Model\n\n\n\nCode\ntargets = pd.read_parquet(\"../testing_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_100/test_results.targets.lstm_20241208_023701.parquet\")\npredictions = pd.read_parquet(\"../testing_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_100/test_results.predictions.lstm_20241208_023701.parquet\")\nsources = pd.read_parquet(\"../testing_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_100/test_results.sources.lstm_20241208_023701.parquet\")\n\ndf_bins = pd.read_parquet(\"./model_input_spec_225.parquet\")\n\n\ntargets['vert_displacement_meters'].plot(figsize=(16, 4), label=\"target\", linewidth = 0.85)\npredictions['vert_displacement_meters'].plot(label=\"prediction\", linewidth=0.75)\nplt.ylabel(\"Vertical Displacement [m]\")\nplt.show()\nresult_stats.append(calc_stats(\"100 Epoch LSTM\", \"225\", targets, predictions))\n\ntargets = pd.read_parquet(\"../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_100/test_results.targets.lstm_20241208_061107.parquet\")\npredictions = pd.read_parquet(\"../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_100/test_results.predictions.lstm_20241208_061107.parquet\")\nsources = pd.read_parquet(\"../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_100/test_results.sources.lstm_20241208_061107.parquet\")\n\ndf_bins = pd.read_parquet(\"./model_input_spec_243.parquet\")\n\n\ntargets['vert_displacement_meters'].plot(figsize=(16, 4), label=\"target\", linewidth = 0.85)\npredictions['vert_displacement_meters'].plot(label=\"prediction\", linewidth=0.75)\nplt.ylabel(\"Vertical Displacement [m]\")\nplt.show()\nresult_stats.append(calc_stats(\"100 Epoch LSTM\", \"243\", targets, predictions))\n\n\n\n\n\n\n\n\n\n\n\n\n(a) CDIP 225\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) CDIP 243\n\n\n\n\n\n\n\nFigure 42: LSTM 100 Epoch - All Test Sections\n\n\n\n\n\n\n\n6.2.4 LSTM 4 layers\n\n\n\nCode\ntargets = pd.read_parquet(\"../testing_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_4.EPOCHS_25/test_results.targets.lstm_20241208_072749.parquet\")\npredictions = pd.read_parquet(\"../testing_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_4.EPOCHS_25/test_results.predictions.lstm_20241208_072749.parquet\")\nsources = pd.read_parquet(\"../testing_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_4.EPOCHS_25/test_results.sources.lstm_20241208_072749.parquet\")\n\ndf_bins = pd.read_parquet(\"./model_input_spec_225.parquet\")\n\n\ntargets['vert_displacement_meters'].plot(figsize=(16, 4), label=\"target\", linewidth = 0.85)\npredictions['vert_displacement_meters'].plot(label=\"prediction\", linewidth=0.75)\nplt.ylabel(\"Vertical Displacement [m]\")\nplt.show()\nresult_stats.append(calc_stats(\"4 Layer LSTM\", \"225\", targets, predictions))\n\ntargets = pd.read_parquet(\"../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_4.EPOCHS_25/test_results.targets.lstm_20241208_083208.parquet\")\npredictions = pd.read_parquet(\"../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_4.EPOCHS_25/test_results.predictions.lstm_20241208_083208.parquet\")\nsources = pd.read_parquet(\"../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_4.EPOCHS_25/test_results.sources.lstm_20241208_083208.parquet\")\n\ndf_bins = pd.read_parquet(\"./model_input_spec_243.parquet\")\n\n\ntargets['vert_displacement_meters'].plot(figsize=(16, 4), label=\"target\", linewidth = 0.85)\npredictions['vert_displacement_meters'].plot(label=\"prediction\", linewidth=0.75)\nplt.ylabel(\"Vertical Displacement [m]\")\nplt.show()\nresult_stats.append(calc_stats(\"4 Layer LSTM\", \"243\", targets, predictions))\n\n\n\n\n\n\n\n\n\n\n\n\n(a) CDIP 225\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) CDIP 243\n\n\n\n\n\n\n\nFigure 43: LSTM 4 Layer - All Test Sections\n\n\n\n\n\n\n\n6.2.5 LSTM 6 Layer\n\n\n\nCode\ntargets = pd.read_parquet(\"../testing_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_6.EPOCHS_25/test_results.targets.lstm_20241208_093344.parquet\")\npredictions = pd.read_parquet(\"../testing_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_6.EPOCHS_25/test_results.predictions.lstm_20241208_093344.parquet\")\nsources = pd.read_parquet(\"../testing_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_6.EPOCHS_25/test_results.sources.lstm_20241208_093344.parquet\")\n\ndf_bins = pd.read_parquet(\"./model_input_spec_225.parquet\")\n\n\ntargets['vert_displacement_meters'].plot(figsize=(16, 4), label=\"target\", linewidth = 0.85)\npredictions['vert_displacement_meters'].plot(label=\"prediction\", linewidth=0.75)\nplt.ylabel(\"Vertical Displacement [m]\")\nplt.show()\n\nresult_stats.append(calc_stats(\"6 Layer LSTM\", \"225\", targets, predictions))\n\ntargets = pd.read_parquet(\"../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_6.EPOCHS_25/test_results.targets.lstm_20241208_110346.parquet\")\npredictions = pd.read_parquet(\"../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_6.EPOCHS_25/test_results.predictions.lstm_20241208_110346.parquet\")\nsources = pd.read_parquet(\"../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_6.EPOCHS_25/test_results.sources.lstm_20241208_110346.parquet\")\n\ndf_bins = pd.read_parquet(\"./model_input_spec_243.parquet\")\n\n\ntargets['vert_displacement_meters'].plot(figsize=(16, 4), label=\"target\", linewidth = 0.85)\npredictions['vert_displacement_meters'].plot(label=\"prediction\", linewidth=0.75)\nplt.ylabel(\"Vertical Displacement [m]\")\nplt.show()\nresult_stats.append(calc_stats(\"6 Layer LSTM\", \"243\", targets, predictions))\n\n\n\n\n\n\n\n\n\n\n\n\n(a) CDIP 225\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) CDIP 243\n\n\n\n\n\n\n\nFigure 44: LSTM 6 Layer - All Test Sections\n\n\n\n\n\n\n\n6.2.6 Enhanced Transformer\n\n\n\nCode\ntargets = pd.read_parquet(\"../testing_history/model_enhanced_transformer.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.targets.enhanced_transformer_20241208_121422.parquet\")\npredictions = pd.read_parquet(\"../testing_history/model_enhanced_transformer.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.predictions.enhanced_transformer_20241208_121422.parquet\")\nsources = pd.read_parquet(\"../testing_history/model_enhanced_transformer.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.sources.enhanced_transformer_20241208_121422.parquet\")\n\ndf_bins = pd.read_parquet(\"./model_input_spec_225.parquet\")\n\n\ntargets['vert_displacement_meters'].plot(figsize=(16, 4), label=\"target\", linewidth = 0.85)\npredictions['vert_displacement_meters'].plot(label=\"prediction\", linewidth=0.75)\nplt.ylabel(\"Vertical Displacement [m]\")\nplt.show()\n\nresult_stats.append(calc_stats(\"Enhanced Transformer\", \"225\", targets, predictions))\n\ntargets = pd.read_parquet(\"../testing_history/model_enhanced_transformer.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.targets.enhanced_transformer_20241208_125950.parquet\")\npredictions = pd.read_parquet(\"../testing_history/model_enhanced_transformer.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.predictions.enhanced_transformer_20241208_125950.parquet\")\nsources = pd.read_parquet(\"../testing_history/model_enhanced_transformer.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.sources.enhanced_transformer_20241208_125950.parquet\")\n\ndf_bins = pd.read_parquet(\"./model_input_spec_243.parquet\")\n\n\ntargets['vert_displacement_meters'].plot(figsize=(16, 4), label=\"target\", linewidth = 0.85)\npredictions['vert_displacement_meters'].plot(label=\"prediction\", linewidth=0.75)\nplt.ylabel(\"Vertical Displacement [m]\")\nplt.show()\nresult_stats.append(calc_stats(\"Enhanced Transformer\", \"243\", targets, predictions))\n\n\n\n\n\n\n\n\n\n\n\n\n(a) CDIP 225\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) CDIP 243\n\n\n\n\n\n\n\nFigure 45: Enhanced Transformer - All Test Sections\n\n\n\n\n\n\n\n6.2.7 Enhanced LSTM\n\n\n\nCode\ntargets = pd.read_parquet(\"../testing_history/model_enhanced_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.targets.enhanced_lstm_20241208_160252.parquet\")\npredictions = pd.read_parquet(\"../testing_history/model_enhanced_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.predictions.enhanced_lstm_20241208_160252.parquet\")\nsources = pd.read_parquet(\"../testing_history/model_enhanced_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.sources.enhanced_lstm_20241208_160252.parquet\")\n\ndf_bins = pd.read_parquet(\"./model_input_spec_225.parquet\")\n\n\n# start = 128\nstart = 0\n# end = start + 128\nend = -1\n\ntargets['vert_displacement_meters'].iloc[start:end].plot(figsize=(16, 4), label=\"target\", linewidth = 0.85)\npredictions['vert_displacement_meters'].iloc[start:end].plot(label=\"prediction\", linewidth=0.75)\nplt.ylabel(\"Vertical Displacement [m]\")\nplt.show()\n\nresult_stats.append(calc_stats(\"Enhanced LSTM\", \"225\", targets, predictions))\n\ntargets = pd.read_parquet(\"../testing_history/model_enhanced_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.targets.enhanced_lstm_20241208_170202.parquet\")\npredictions = pd.read_parquet(\"../testing_history/model_enhanced_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.predictions.enhanced_lstm_20241208_170202.parquet\")\nsources = pd.read_parquet(\"../testing_history/model_enhanced_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.sources.enhanced_lstm_20241208_170202.parquet\")\n\ndf_bins = pd.read_parquet(\"./model_input_spec_243.parquet\")\n\n\ntargets['vert_displacement_meters'].plot(figsize=(16, 4), label=\"target\", linewidth = 0.85)\npredictions['vert_displacement_meters'].plot(label=\"prediction\", linewidth=0.75)\nplt.ylabel(\"Vertical Displacement [m]\")\nplt.show()\nresult_stats.append(calc_stats(\"Enhanced LSTM\", \"243\", targets, predictions))\n\n\n\n\n\n\n\n\n\n\n\n\n(a) CDIP 225\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) CDIP 243\n\n\n\n\n\n\n\nFigure 46: Enhanced LSTM - All Test Sections\n\n\n\n\n\nFigure 40 through Figure 46 present the complete test set predictions against measured displacements, offering a comprehensive view of model performance across all wave conditions. The LSTM-based architectures demonstrate strong predictive capabilities, closely tracking the measured wave displacements across varying sea states. In particular, the baseline, enhanced, 4-layer, and 6-layer LSTM models show consistent prediction accuracy, suggesting successful learning of the underlying wave dynamics across diverse conditions.\nThe visual alignment between predictions and measurements for the LSTM models indicates their ability to capture both the frequency and amplitude characteristics of the wave motion. This comprehensive fit across different wave conditions supports the quantitative metrics and demonstrates the models’ generalization capabilities. In contrast, the Transformer models show notable deviation from the measured displacements, confirming their limited effectiveness for this prediction task."
  },
  {
    "objectID": "index.html#results-comparison",
    "href": "index.html#results-comparison",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "6.3 Results Comparison",
    "text": "6.3 Results Comparison\nWe evaluated model performance using three metrics: Mean Absolute Error (MAE), coefficient of determination (R^2), and Pearson’s correlation coefficient (\\rho).\n\n\nCode\nresults_df = pd.DataFrame(result_stats)\n\n\n\n6.3.1 Mean Absolute Error\n\n\nCode\nresults_df = results_df.sort_values([\"mae\", \"station\"])\n\nplt.figure(figsize=(6, 3.5))\nsns.barplot(results_df, y=\"label\", x=\"mae\", hue=\"station\")\nfor i in plt.gca().containers:\n    plt.bar_label(i, fmt='%.2f', padding=3)\nplt.ylabel(None);\nplt.xlabel(\"Mean Absolute Error\");\n\n\n\n\n\n\n\n\nFigure 47: Mean Absolute Error Comparison by Model (Lower is better)\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(8, 4.0))\nsns.barplot(results_df, y=\"station\", x=\"mae\", hue=\"label\")\nfor i in plt.gca().containers:\n    plt.bar_label(i, fmt='%.2f', padding=3)\nplt.ylabel(None);\nplt.xlabel(\"Mean Absolute Error\");\nplt.legend(bbox_to_anchor=(1.25, 1.0),\n          loc='upper center',\n          ncol=1,\n          frameon=False, title=\"Model\")\n\n\n\n\n\n\n\n\nFigure 48: Mean Absolute Error Comparison by Station (Lower is better)\n\n\n\n\n\n\n\n\n6.3.2 Coefficient of Determination, R^2\n\n\nCode\nresults_df = results_df.sort_values([\"r2\"], ascending=False)\n\nplt.figure(figsize=(6, 3.5))\nsns.barplot(results_df, y=\"label\", x=\"r2\", hue=\"station\")\nfor i in plt.gca().containers:\n    plt.bar_label(i, fmt='%.2f', padding=3)\nplt.ylabel(None);\nplt.xlabel(\"$R^2$\");\n\n\n\n\n\n\n\n\nFigure 49: R^2 Comparison by Model\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(8, 4.0))\nsns.barplot(results_df, y=\"station\", x=\"r2\", hue=\"label\")\nfor i in plt.gca().containers:\n    plt.bar_label(i, fmt='%.2f', padding=3)\nplt.ylabel(None);\nplt.xlabel(\"$R^2$\");\nplt.legend(bbox_to_anchor=(1.25, 1.0),\n          loc='upper center',\n          ncol=1,\n          frameon=False, title=\"Model\")\n\n\n\n\n\n\n\n\nFigure 50: R^2 Comparison by Station\n\n\n\n\n\n\n\n\n6.3.3 Pearson’s Correlation [\\rho]\n\n\nCode\nresults_df = results_df.sort_values([\"correlation\"], ascending=False)\n\nplt.figure(figsize=(6, 3.5))\nsns.barplot(results_df, y=\"label\", x=\"correlation\", hue=\"station\")\nfor i in plt.gca().containers:\n    plt.bar_label(i, fmt='%.2f', padding=3)\nplt.ylabel(None);\nplt.xlabel(\"Correlation\");\n\n\n\n\n\n\n\n\nFigure 51: Pearson’s Correlation Comparison by Model\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(8, 4.0))\nsns.barplot(results_df, y=\"station\", x=\"correlation\", hue=\"label\")\nfor i in plt.gca().containers:\n    plt.bar_label(i, fmt='%.2f', padding=3)\nplt.ylabel(None);\nplt.xlabel(\"Correlation\");\nplt.legend(bbox_to_anchor=(1.25, 1.0),\n          loc='upper center',\n          ncol=1,\n          frameon=False, title=\"Model\")\n\n\n\n\n\n\n\n\nFigure 52: Pearson’s Correlation Comparison by Station\n\n\n\n\n\n\n\n\nCode\nresults_df\n\n\n\n\nTable 4: Results Summary\n\n\n\n\n\n\n\n\n\n\nlabel\nstation\nmae\nr2\ncorrelation\n\n\n\n\n6\n4 Layer LSTM\n225\n0.502155\n0.889149\n0.942949\n\n\n7\n4 Layer LSTM\n243\n0.874835\n0.868795\n0.932219\n\n\n4\n100 Epoch LSTM\n225\n0.582889\n0.866286\n0.930816\n\n\n8\n6 Layer LSTM\n225\n0.565404\n0.856992\n0.925749\n\n\n9\n6 Layer LSTM\n243\n0.946516\n0.851715\n0.922910\n\n\n12\nEnhanced LSTM\n225\n0.699822\n0.837119\n0.915022\n\n\n0\nBaseline\n225\n0.692493\n0.829845\n0.911056\n\n\n5\n100 Epoch LSTM\n243\n1.139891\n0.807409\n0.898738\n\n\n13\nEnhanced LSTM\n243\n1.223194\n0.789425\n0.888552\n\n\n1\nBaseline\n243\n1.235250\n0.788793\n0.888162\n\n\n11\nEnhanced Transformer\n243\n2.793854\n0.004303\n0.076359\n\n\n2\nTransformer\n225\n1.880350\n0.002838\n0.072718\n\n\n10\nEnhanced Transformer\n225\n1.932608\n-0.047880\n0.019221\n\n\n3\nTransformer\n243\n2.797447\n-0.000079\n0.005579"
  },
  {
    "objectID": "index.html#results-summary",
    "href": "index.html#results-summary",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "6.4 Results Summary",
    "text": "6.4 Results Summary\nWe evaluated model performance using three metrics: Mean Absolute Error (MAE), coefficient of determination (R^2), and Pearson’s correlation coefficient (\\rho). As shown in Figure 47 through Figure 51, the LSTM-based models consistently outperformed the Transformer architectures across all metrics. The 4-layer LSTM achieved the best performance with MAE of 0.50\\,\\mathrm{m} and 0.87\\,\\mathrm{m} for WETS and Nags Head respectively, along with the highest R^2 values (0.89, 0.87) and correlation coefficients (0.94, 0.93). Increasing training epochs from 25 to 100 improved the baseline LSTM’s performance, particularly for WETS where the MAE decreased from 0.69\\,\\mathrm{m} to 0.58\\,\\mathrm{m}. However, further increasing model depth to 6 layers showed no additional benefit.\nBoth the basic and enhanced Transformer models performed poorly, with R^2 values near zero and correlation coefficients below 0.1, suggesting they failed to capture meaningful wave patterns. The enhanced LSTM showed no improvement over the baseline model, indicating that the additional architectural complexity did not benefit the wave prediction task.\nAcross all models, prediction accuracy was consistently better for WETS compared to Nags Head, as shown in Figure 48. This aligns with our earlier observation that WETS exhibits more regular wave patterns, while Nags Head experiences more variable conditions."
  },
  {
    "objectID": "index.html#key-findings",
    "href": "index.html#key-findings",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "7.1 Key Findings",
    "text": "7.1 Key Findings\nThe LSTM-based models consistently outperformed Transformer architectures, with the 4-layer LSTM achieving the best results (\\mathrm{MAE} = 0.50\\,\\mathrm{m} at WETS). Model performance showed sensitivity to architecture choices, where increasing complexity beyond four layers provided diminishing returns. Prediction accuracy was generally better at WETS than Nags Head, reflecting the more regular wave patterns at the Hawaiian site. While our models showed good prediction capabilities, some evidence of overfitting suggests room for improvement in model regularization."
  },
  {
    "objectID": "index.html#lessons-learned",
    "href": "index.html#lessons-learned",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "7.2 Lessons Learned",
    "text": "7.2 Lessons Learned\nThe temporal nature of wave prediction requires careful consideration of model architecture and data preparation. LSTM networks proved particularly effective at capturing wave patterns, while Transformer models, despite their success in other sequential tasks, failed to capture meaningful wave dynamics. Increasing training epochs consistently improved performance, suggesting that extended training periods might yield further improvements. The structured sampling approach across sea states proved valuable for creating representative training datasets."
  },
  {
    "objectID": "index.html#future-work",
    "href": "index.html#future-work",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "7.3 Future Work",
    "text": "7.3 Future Work\nSeveral promising directions for future research emerge from this work. Extension to north and east displacement predictions would provide a more complete wave motion model. Physics-informed neural networks could incorporate wave dynamics directly into the learning process. Alternative data sampling strategies and real-time validation would enhance model robustness. Additional coastal locations would test model generalization, while optimized scaling methods might improve prediction accuracy. These enhancements could lead to more reliable wave prediction systems for maritime applications."
  }
]