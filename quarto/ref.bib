@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@misc{tensorflow,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@misc{keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@inproceedings{pytorch,
author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, CK and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Suo, Michael and Tillet, Phil and Wang, Eikan and Wang, Xiaodong and Wen, William and Zhang, Shunting and Zhao, Xu and Zhou, Keren and Zou, Richard and Mathews, Ajit and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
booktitle = {29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24)},
doi = {10.1145/3620665.3640366},
month = apr,
publisher = {ACM},
title = {{PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation}},
url = {https://pytorch.org/assets/pytorch2-2.pdf},
year = {2024}
}

@book{Minsky_Perceptrons,
    author = {Minsky, Marvin and Papert, Seymour A.},
    title = "{Perceptrons: An Introduction to Computational Geometry}",
    publisher = {The MIT Press},
    year = {2017},
    month = {09},
    abstract = "{The first systematic study of parallelism in computation by two pioneers in the field.Reissue of the 1988 Expanded Edition with a new foreword by Léon BottouIn 1969, ten years after the discovery of the perceptron—which showed that a machine could be taught to perform certain tasks using examples—Marvin Minsky and Seymour Papert published Perceptrons, their analysis of the computational capabilities of perceptrons for specific tasks. As Léon Bottou writes in his foreword to this edition, “Their rigorous work and brilliant technique does not make the perceptron look very good.” Perhaps as a result, research turned away from the perceptron. Then the pendulum swung back, and machine learning became the fastest-growing field in computer science. Minsky and Papert's insistence on its theoretical foundations is newly relevant.Perceptrons—the first systematic study of parallelism in computation—marked a historic turn in artificial intelligence, returning to the idea that intelligence might emerge from the activity of networks of neuron-like entities. Minsky and Papert provided mathematical analysis that showed the limitations of a class of computing machines that could be considered as models of the brain. Minsky and Papert added a new chapter in 1987 in which they discuss the state of parallel computers, and note a central theoretical challenge: reaching a deeper understanding of how “objects” or “agents” with individuality can emerge in a network. Progress in this area would link connectionism with what the authors have called “society theories of mind.”}",
    isbn = {9780262343930},
    doi = {10.7551/mitpress/11301.001.0001},
    url = {https://doi.org/10.7551/mitpress/11301.001.0001},
}

@ARTICLE{Lecun_CNN,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE},
  title={Gradient-based learning applied to document recognition},
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  keywords={Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
  doi={10.1109/5.726791}}


@misc{Simonyan_Deep_CNN,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1409.1556},
}

@misc{He_Deep_Residual_Learning,
      title={Deep Residual Learning for Image Recognition},
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385},
}

@misc{Szegedy_InceptionNet,
      title={Going Deeper with Convolutions},
      author={Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
      year={2014},
      eprint={1409.4842},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1409.4842},
}

@misc{kaggle_cancer,
    author = {Will Cukierski},
    title = {Histopathologic Cancer Detection},
    year = {2018},
    howpublished = {\url{https://kaggle.com/competitions/histopathologic-cancer-detection}},
    note = {Kaggle}
}

@misc{kaggle_nlp,
    author = {Addison Howard and devrishi and Phil Culliton and Yufeng Guo},
    title = {Natural Language Processing with Disaster Tweets},
    year = {2019},
    howpublished = {\url{https://kaggle.com/competitions/nlp-getting-started}},
    note = {Kaggle}
}

@book{py_lib_nltk,
author = {Bird, Steven and Klein, Ewan and Loper, Edward},
isbn = {9780596516499},
month = jun,
publisher = {O'Reilly Media, Inc.},
title = {{Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit}},
url = {https://www.nltk.org/book/},
year = {2009}
}

@article{LSTM_paper,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = {Long Short-Term Memory},
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@software{transformers_python_lib,
  author       = {Wolf, Thomas and
                  Debut, Lysandre and
                  Sanh, Victor and
                  Chaumond, Julien and
                  Delangue, Clement and
                  Moi, Anthony and
                  Cistac, Perric and
                  Ma, Clara and
                  Jernite, Yacine and
                  Plu, Julien and
                  Xu, Canwen and
                  Le Scao, Teven and
                  Gugger, Sylvain and
                  Drame, Mariama and
                  Lhoest, Quentin and
                  Rush, Alexander M.},
  title        = {{Transformers: State-of-the-Art Natural Language
                   Processing}},
  month        = dec,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {v4.25.1},
  doi          = {10.5281/zenodo.7391177},
  url          = {https://doi.org/10.5281/zenodo.7391177}
}

@misc{kaggle_gan_monet,
    author = {Amy Jang and Ana Sofia Uzsoy and Phil Culliton},
    title = {I’m Something of a Painter Myself},
    year = {2020},
    howpublished = {\url{https://kaggle.com/competitions/gan-getting-started}},
    note = {Kaggle}
}

@misc{patch_gan,
      title={Image-to-Image Translation with Conditional Adversarial Networks},
      author={Phillip Isola and Jun-Yan Zhu and Tinghui Zhou and Alexei A. Efros},
      year={2018},
      eprint={1611.07004},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1611.07004},
}

@dataset{cdip_wets,
    title        = {Station 225: Kaneohe Bay, WETS, HI - Wave, Sea Surface Temperature, and Ocean Current Time-Series Data},
    author       = {{Coastal Data Information Program}},
    year         = {2023},
    institution  = {Scripps Institution of Oceanography},
    publisher    = {UC San Diego Library Digital Collections},
    doi          = {10.18437/C7WC72},
    url          = {https://cdip.ucsd.edu/m/products/?stn=225p1},
    note         = {Station coordinates: 21°28.644'N 157°45.410'W, Active since August 2016},
    version      = {1.0},
    organization = {USACE/PACIOOS}
}

@dataset{cdip_nags,
    title        = {Station 243: Nags Head, NC - Wave, Sea Surface Temperature, and Ocean Current Time-Series Data},
    author       = {{Coastal Data Information Program}},
    year         = {2023},
    institution  = {Scripps Institution of Oceanography},
    publisher    = {UC San Diego Library Digital Collections},
    doi          = {10.18437/C7WC72},
    url          = {https://cdip.ucsd.edu/m/products/?stn=243p1},
    note         = {Station coordinates: 36°0.090'N 75°25.254'W, Active since August 2018},
    version      = {1.0},
    organization = {USACE/CSI}
}

@book{holthuijsen2010waves,
    title={Waves in Oceanic and Coastal Waters},
    author={Holthuijsen, Leo H},
    year={2010},
    publisher={Cambridge University Press}
}

@book{tucker2001waves,
    title={Waves in Ocean Engineering: Measurement, Analysis, Interpretation},
    author={Tucker, M.J. and Pitt, E.G.},
    year={2001},
    publisher={Elsevier}
}

@book{ochi1998ocean,
    title={Ocean Waves: The Stochastic Approach},
    author={Ochi, Michel K},
    year={1998},
    publisher={Cambridge University Press}
}

@manual{datawell2006manual,
    title={Datawell Waverider Reference Manual},
    author={{Datawell BV}},
    year={2006},
    organization={Datawell BV Oceanographic Instruments}
}

@software{mhkit_python,
  author       = {Rebecca Fao and
                  Sterling Olson and
                  Katherine Klise and
                  Kelley Ruehl and
                  Chris-Ivanov and
                  Ryan Coe and
                  Carlos A. Michelén Ströfer and
                  James McVey and
                  Adam Keester and
                  Emily Browning and
                  Matthew Boyd and
                  Bax and
                  Mark Bruggemann and
                  Calum Kenny and
                  aidanbharath and
                  Andrew Simms and
                  Cesar and
                  Dylan Mayes and
                  Maxwell Levin and
                  Sean Pluemer},
  title        = {MHKiT-Software/MHKiT-Python: v0.8.2},
  month        = aug,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.8.2},
  doi          = {10.5281/zenodo.13320100},
  url          = {https://doi.org/10.5281/zenodo.13320100}
}

@article{ringwood2014wave,
title = {Wave Energy Control Systems: Robustness Issues⁎⁎This work was supported by Science Foundation Ireland under Grant No. SFI/13/IA/1886 and Grant No. 12/RC/2302 for the Marine Renewable Ireland (MaREI) centre.},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {29},
pages = {62-67},
year = {2018},
note = {11th IFAC Conference on Control Applications in Marine Systems, Robotics, and Vehicles CAMS 2018},
issn = {2405-8963},
doi = {10.1016/j.ifacol.2018.09.470},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318321591},
author = {John V. Ringwood and Alexis Merigaud and Nicolás Faedo and Francesco Fusco},
keywords = {Wave energy, control system, sensitivity, robustness, power maximisation},
abstract = {While traditional feedback control systems enjoy relatively good sensitivity properties, energy maximising wave energy converter (WEC) control systems have particular characteristics which challenge the application of traditional feedback and robust control methods. In particular, the relationship between plant and controller is largely defined by the need to maximise power transfer, and the controller contains a feedforward component which is difficult to robustify. Typically, WEC control systems are based on linear model descriptions, but this belies the true nonlinearity of WEC hydrodynamics (particularly under controlled conditions) and the associated power take-off (PTO) system. This paper examines two popular WEC control structures and examines the sensitivity of these structures to parameter variations, both in terms of closed-loop transfer functions and power absorbed. Some recommendations are also given on which WEC parameters need to be modelled with high accuracy.}
}

@inbook{dean2004coastal,
    place={Cambridge},
    title={Waves and Wave-Induced Hydrodynamics},
    booktitle={Coastal Processes with Engineering Applications},
    publisher={Cambridge University Press},
    author={Dean, Robert G. and Dalrymple, Robert A.},
    year={2001},
    pages={88–130}
}

@article{mandal2006ocean,
title = {Ocean wave forecasting using recurrent neural networks},
journal = {Ocean Engineering},
volume = {33},
number = {10},
pages = {1401-1410},
year = {2006},
issn = {0029-8018},
doi = {10.1016/j.oceaneng.2005.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0029801805002301},
author = {S. Mandal and N. Prabaharan},
keywords = {Wave forecasting, NARX recurrent network, Correlation coefficients},
abstract = {The tremendous increase in offshore operational activities demands improved wave forecasting techniques. With the knowledge of accurate wave conditions, it is possible to carry out the marine activities such as offshore drilling, naval operations, merchant vessel routing, nearshore construction, etc. more efficiently and safely. This paper describes an artificial neural network, namely recurrent neural network with rprop update algorithm and is applied for wave forecasting. Measured ocean waves off Marmugao, west coast of India are used for this study. Here, the recurrent neural network of 3, 6 and 12 hourly wave forecasting yields the correlation coefficients of 0.95, 0.90 and 0.87, respectively. This shows that the wave forecasting using recurrent neural network yields better results than the previous neural network application.}
}


@misc{james2017machinelearningframeworkforecast,
      title={A Machine Learning Framework to Forecast Wave Conditions},
      author={Scott C. James and Yushan Zhang and Fearghal O'Donncha},
      year={2017},
      eprint={1709.08725},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph},
      url={https://arxiv.org/abs/1709.08725},
}

@article{ringwood_2020,
title = {Wave energy control: status and perspectives 2020 ⁎⁎This paper is based upon work supported by Science Foundation Ireland under Grant no. 13/IA/1886 and Grant No. 12/RC/2302 for the Marine Renewable Ireland (MaREI) centre.},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {12271-12282},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {10.1016/j.ifacol.2020.12.1162},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320315536},
author = {John V. Ringwood},
keywords = {Wave energy, modelling, control, estimation, forecasting, sensitivity, robust control, nonlinearity},
abstract = {Wave energy has a significant part to play in providing a carbon-free solution to the world’s increasing appetite for energy. In many countries, there is sufficient wave energy to cater for the entire national demand, and wave energy also has some attractive features in being relatively uncorrelated with wind, solar and tidal energy, easing the renewable energy dispatch problem. However, wave energy has not yet reached commercial viability, despite the first device designs being proposed in 1898. Control technology can play a major part in the drive for economic viability of wave energy and this paper charts the progress made since the first wave energy control systems were suggested in the 1970s, and examines current outstanding challenges for the control community.}
}

@article{abd_2016,
	abstract = {This paper presents a shape-based approach to compute, in an optimal sense, the control of a single degree-of-freedom point absorber wave energy converter. In this study, it is assumed that a prediction for the wave is available. The control is computed so as to maximize the energy extraction over a future time horizon. In the shape-base approach, one of the system states is represented by a series expansion. The optimization variables are selected to be the coefficients in the series expansion instead of the history of the control variable. A gradient-based optimizer is used to optimize the series coefficients. The available wave prediction is used to compute an initial guess for the unknown series coefficients. This concept is tested on two different dynamic models: a simplified reduced-order model and a dynamic model with radiation dynamic states necessary to compute the radiation force. Several test cases are presented that cover a range of different sea states. The results show that the shape-based approach finds efficient solutions in terms of the extracted energy. The results also show that the obtained solutions are suitable for real-time implementation in terms of the smoothness of the obtained control and the speed of computations. Comparisons between the results of the shape-based approach and other techniques are presented and discussed.},
	author = {Abdelkhalik, Ossama and Robinett, Rush and Zou, Shangyan and Bacelli, Giorgio and Coe, Ryan and Bull, Diana and Wilson, David and Korde, Umesh},
	date = {2016/11/01},
	date-added = {2024-12-08 11:52:32 -0700},
	date-modified = {2024-12-08 11:52:32 -0700},
	doi = {10.1007/s40722-016-0048-4},
	id = {Abdelkhalik2016},
	isbn = {2198-6452},
	journal = {Journal of Ocean Engineering and Marine Energy},
	number = {4},
	pages = {473--483},
	title = {On the control design of wave energy converters with wave prediction},
	url = {10.1007/s40722-016-0048-4},
	volume = {2},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1007/s40722-016-0048-4}}

    @article{kumar_ocean_sequential_nn,
title = {Regional ocean wave height prediction using sequential learning neural networks},
journal = {Ocean Engineering},
volume = {129},
pages = {605-612},
year = {2017},
issn = {0029-8018},
doi = {10.1016/j.oceaneng.2016.10.033},
url = {https://www.sciencedirect.com/science/article/pii/S0029801816304796},
author = {N. Krishna Kumar and R. Savitha and Abdullah Al Mamun},
keywords = {GAP-RBF, Minimal resource allocation network, Sequential learning, Prediction, Ocean wave height},
abstract = {Wave height prediction is a critical factor in the efficient operation of many offshore and coastal engineering activities. The classical numerical solutions to this problem, based on the wave energy-balance equation, involves complex implementation with higher computational powers. In recent years, machine learning approaches are being widely used for the prediction of wave heights. However, these approaches involve batch learning algorithms that are not well-equipped to address the demands of continuously changing data stream. In this paper, we conduct a study to predict the daily wave heights in different geographical regions using sequential learning algorithms, namely the Minimal Resource Allocation Network (MRAN) and the Growing and Pruning Radial Basis Function (GAP-RBF) network. The study is conducted using data collected from 13 stations across three geographically distinct regions, viz., the Gulf of Mexico, the Korean region and the UK region, for the period between Jan 1, 2011 and Aug 30, 2015. The data is chosen such that the study covers a wide range of geographical terrains and locations, a wide range of wind speed and wave heights. We compare the performance of MRAN and GAP-RBF with Support Vector Regression (SVR) and Extreme Learning Machine (ELM). The performance study results show that the MRAN and GAP-RBF outperform the SVR and ELM with minimal network resources, in the daily wave height prediction. They also predict the significant wave heights accurately. Performance comparison between MRAN and GAP-RBF shows that MRAN outperforms GAP-RBF with minimal architecture.}
}

@misc{attention_paper,
      title={Attention Is All You Need},
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762},
}

@misc{pytorch_lstm_ref,
      title={Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition},
      author={Haşim Sak and Andrew Senior and Françoise Beaufays},
      year={2014},
      eprint={1402.1128},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1402.1128},
}

@software{pytorch_lightning,
author = {Falcon, William and {The PyTorch Lightning team}},
doi = {10.5281/zenodo.3828935},
license = {Apache-2.0},
month = mar,
title = {{PyTorch Lightning}},
url = {https://github.com/Lightning-AI/lightning},
version = {1.4},
year = {2019}
}

@article{xarray_repo,
author = {Hoyer, Stephan and Joseph, Hamman},
doi = {10.5334/jors.148},
journal = {Journal of Open Research Software},
month = apr,
number = {1},
title = {{xarray: N-D labeled Arrays and Datasets in Python}},
volume = {5},
year = {2017}
}

@software{pandas_repo,
author = {{The pandas development team}},
doi = {10.5281/zenodo.3509134},
license = {BSD-3-Clause},
title = {{pandas-dev/pandas: Pandas}},
url = {https://github.com/pandas-dev/pandas}
}

 @misc{ wiki:lstm_cell,
   author = {{Wikimedia Commons}},
   title = "File:The LSTM Cell.svg --- Wikimedia Commons{,} the free media repository",
   year = "2024",
   url = "https://commons.wikimedia.org/w/index.php?title=File:The_LSTM_Cell.svg&oldid=928752394",
   note = "[Online; accessed 9-December-2024]"
 }
