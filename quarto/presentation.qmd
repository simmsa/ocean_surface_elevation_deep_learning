---
title: DTSA 5511 Final Presentation
subtitle: Using Deep Learning to Model Ocean Surface Elevation using Real World Data
author:
    - name: Andrew Simms
      affiliation: University of Colorado Boulder

date: today

format:
    revealjs:
      theme: [simple, custom-presentation.scss]
      html-math-method: "katex"
      highlight-style: monokai
      chalkboard: false
      fig-format: svg
      # embed-resources: true
      # self-contained-math: true

tbl-cap-location: bottom
lst-cap-location: bottom
bibliography: ref.bib
---

# Introduction


:::: {.columns}

::: {.column width="50%"}

* Improved ocean surface elevation prediction could benefit ocean engineering and wave energy
* Project uses deep learning to forecast surface elevation from buoy data
* Goal is accurate real-time prediction to improve:
  * Ocean Structure Designs
    * Fatigue
    * Safety Factors
  * Wave Energy Converters
    * Capture Efficiency
    * Fatigue
* Presentation is a summary of results
    * Full resorts in report

:::

::: {.column width="50%"}

![Datawell Mark III Wave Measurement Instrument Deployed by CDIP. Photo by [PacIOOS/Kimball Millikan](https://www.pacioos.hawaii.edu/metadata/cdip_202.html)](./img/cdip_202.2.png){#fig-cdi4-202 height=500px}

:::

:::

# Locations

:::: {.columns}

::: {.column width="50%"}

* Two strategic sites chosen for long-term data and seasonal variations

* Kaneohe Bay, Hawaii (CDIP 225):
    * Deep water site (84m) at Navy's Wave Energy Test Site
    * Mixed climate with trade winds and Pacific swells
    * Consistent conditions, data since 2012

* Nags Head, North Carolina (CDIP 243):
    * Intermediate depth (21m) near Jennette's Pier Wave Energy Center
    * Wind-driven waves with high variability from Cape Hatteras weather
    * Experiences severe storms and tropical cyclones, data since 2017

:::

::: {.column width="50%"}


```{python}
#| label: fig-wave-wets-map
#| fig-cap: Map of CDIP Buoy 225 Location at the Wave Energy Test Site, Kaneohe Bay, Oahu, HI

import folium

def create_location_map(latitude, longitude, label, zoom_level=8):
    location_map = folium.Map(location=[latitude, longitude], zoom_start=zoom_level)

    folium.Marker(
        [latitude, longitude],
        popup=label,
        icon=folium.Icon(color="red", icon="info-sign"),
    ).add_to(location_map)

    return location_map

wets_map = create_location_map(
    latitude=21.47740,
    longitude=-157.75684,
    label='CDIP 225 - WETS Hawaii'
)
wets_map
```


```{python}
#| label: fig-wave-nags-head-map
#| fig-cap: Map of CDIP Buoy 243 Location in Nags Head, NC

nags_head_map = create_location_map(
    latitude= 36.00150,
    longitude=-75.42090,
    label='CDIP 243 - Nags Head NC'
)
nags_head_map
```

:::

::::


# Data Acquisition Hardware

:::{.columns}

::: {.column width="50%"}


* Datawell Waverider DWR-MkIII buoys sample at 1.28 Hz
* Captures 3D displacement measurements for wave motion
* Data undergoes CDIP quality control and processing

* Tracks buoy movement in three dimensions:
  - Vertical (Z)
  - East-West (X)
  - North-South (Y)

* Aim is to forecast real-time displacements using historical data from all axes

:::

::: {.column width="50%"}

![CDIP Buoy 204 Deployed in Lower Cook Inlet Alaska. Photo from [AOOS](https://legacy.aoos.org/cook-inlet-wave-buoy-up-and-running/)](./img/Wave_buoy_Cook_Inlet_2011_low_res-1024x627.jpg){#fig-cdip-cook-inlet width=350px}

![Directional Reference Frame of Datawell Waverider DWR-MkIII Buoy](./img/buoy-reference_frame.svg){#fig-buoy-movement width=350px}


:::

:::

# Deep Learning Architecture Overview {#sec-dl-overview}

:::{.columns}

::: {.column width="50%"}

**Model Architecture:**

* Uses two approaches: Long Short-Term Memory (LSTM) networks introduced by @LSTM_paper and Transformers by @attention_paper
* LSTM captures long-term dependencies in temporal sequences
* Transformers process sequential data using self-attention mechanisms

**Implementation:**

* Built using PyTorch [@pytorch] framework
* Builds on previous wave forecasting work with neural networks
* Enables direct comparison between LSTM and Transformer performance

:::

::: {.column width="50%"}

![LSTM Cell Visualization. From @wiki:lstm_cell](./img/LSTM_cell.svg){#fig-lstm-cell}

![Transformer Architecture Drawing from @LSTM_paper](./img/transformer_arch.png){#fig-trans-arch height=200px}


:::

:::


# Exploratory Data Analysis - Displacement

:::{.columns}

::: {.column width="40%"}


* **Data Collection & Processing:**
    * CDIP buoys use integrated sensor arrays (accelerometers, magnetometers, GPS)
    * Captures high-resolution 3D wave motion in real-time
    * @fig-displacement shows typical wave patterns (CDIP 225, Nov 2017)

* **Data Characteristics:**
    * Three displacement components: vertical, N/S, E/W
    * Non-linear wave patterns evident in measurements
    * Well-suited for deep learning modeling approaches

:::

::: {.column width="60%"}

```{python}
#| label: fig-displacement
#| fig-cap: "CDIP 225 30 minutes of Displacement - November 11, 2017 @ 11 am. Data from @cdip_wets"
#| fig-subcap:
#|   - Vertical (Z) Displacement
#|   - North/South (Y) Displacement
#|   - East/West (X) Displacement
#| layout-nrow: 3

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

sns.set_theme()
sns.set(font_scale=0.40)

df = pd.read_parquet("../data/a2_std_partition/station_number=0225/year=2017/month=11/day=11/hour=11/minute=00/data_20171111_1100.parquet")

figsize=(6, 1.25)

df['vert_displacement_meters'].plot(figsize=figsize, linewidth=0.65, xlabel="Time", ylabel="Vertical\nDisplacement [m]")
plt.show()

df['north_displacement_meters'].plot(figsize=figsize, linewidth=0.65, xlabel="Time", ylabel="North/South\nDisplacement [m]")
plt.show()

df['east_displacement_meters'].plot(figsize=figsize, linewidth=0.65, xlabel="Time", ylabel="East/West\nDisplacement [m]")
plt.show()
```

:::

:::

# EDA - Displacement Zoomed


```{python}
#| label: fig-displacement-zoomed
#| fig-cap: "CDIP 225 1.5 minutes of Displacement - November 11, 2017 @ 11 am. Data from @cdip_wets"
#| fig-subcap:
#|   - Vertical (Z) Displacement
#|   - North/South (Y) Displacement
#|   - East/West (X) Displacement
#| layout-nrow: 3

sns.set(font_scale=0.60)
df = pd.read_parquet("../data/a2_std_partition/station_number=0225/year=2017/month=11/day=11/hour=11/minute=00/data_20171111_1100.parquet")

end_index = int(288 / 2) # 2304 / 2 / 2 / 2 - ~3 minutes / 2 = 90 seconds

figsize=(12, 1.5)

df['vert_displacement_meters'].iloc[:end_index].plot(figsize=figsize, linewidth=0.5, xlabel="Time", ylabel="Vertical\nDisplacement [m]", marker=".", markersize=3)
plt.xlabel(None)
plt.show()

df['north_displacement_meters'].iloc[:end_index].plot(figsize=figsize, linewidth=0.5, xlabel="Time", ylabel="North/South\nDisplacement [m]", marker=".", markersize=3)
plt.xlabel(None)
plt.show()
df['east_displacement_meters'].iloc[:end_index].plot(figsize=figsize, linewidth=0.5, xlabel="Time", ylabel="East/West\nDisplacement [m]", marker=".", markersize=3)
plt.xlabel(None)
plt.show()
```


# Available Displacement Data

```{python}
import duckdb
import os

def calculate_column_stats(partition_path, column_name):
    con = duckdb.connect()
    con.execute("SET enable_progress_bar = false;")

    query = f"""
    SELECT
        '{column_name}' as column_name,
        COUNT({column_name}) as count,
        COUNT(DISTINCT {column_name}) as unique_count,
        SUM(CASE WHEN {column_name} IS NULL THEN 1 ELSE 0 END) as null_count,
        MIN({column_name}) as min_value,
        MAX({column_name}) as max_value,
        AVG({column_name}::DOUBLE) as mean,
        STDDEV({column_name}::DOUBLE) as std_dev,
        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY {column_name}::DOUBLE) as q1,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY {column_name}::DOUBLE) as median,
        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY {column_name}::DOUBLE) as q3
    FROM read_parquet('{partition_path}/**/*.parquet', hive_partitioning=true)
    WHERE {column_name} IS NOT NULL
    """

    stats_df = con.execute(query).df()
    con.close()
    return stats_df

def analyze_displacement_data(base_path, columns_to_analyze, station_numbers, output_path, overwrite=False):
    # Check if stats file already exists
    if os.path.exists(output_path) and not overwrite:
        return pd.read_parquet(output_path)

    all_stats = []

    for station in station_numbers:
        station_str = f"{station:04d}"  # Format station number with leading zeros
        partition_path = f"{base_path}/station_number={station_str}"

        if not os.path.exists(partition_path):
            print(f"Skipping station {station_str} - path does not exist")
            continue

        print(f"Processing station {station_str}...")

        for column in columns_to_analyze:
            try:
                stats_df = calculate_column_stats(partition_path, column)
                stats_df['station'] = station_str
                all_stats.append(stats_df)
                print(f"  Completed analysis of {column}")
            except Exception as e:
                print(f"  Error processing {column} for station {station_str}: {str(e)}")

    # Combine all results
    if all_stats:
        combined_stats = pd.concat(all_stats, ignore_index=True)

        # Create output directory if it doesn't exist
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        # Save to parquet
        combined_stats.to_parquet(output_path, index=False)
        print(f"\nStatistics saved to {output_path}")
        return combined_stats
    else:
        print("No statistics were generated")
        return None

# Example usage
base_path = "../data/a2_std_partition"
columns_to_analyze = [
    "vert_displacement_meters",
    "north_displacement_meters",
    "east_displacement_meters"
]
station_numbers = [225, 243]  # Add more station numbers as needed
output_path = "../data/displacement_stats.parquet"

# Run the analysis - will load existing file if it exists
stats_df = analyze_displacement_data(
    base_path=base_path,
    columns_to_analyze=columns_to_analyze,
    station_numbers=station_numbers,
    output_path=output_path,
    overwrite=False  # Set to True to force recalculation
)

stats_df["Range [m]"] = stats_df['max_value'] + stats_df['min_value'].abs()

# stats_df.loc[stats_df['column_name'] == 'vert_displacement_meters'] = "Vertical Displacement [m]"
stats_df.loc[stats_df['column_name'] == 'vert_displacement_meters', 'column_name'] = "Vertical Displacement [m]"
stats_df.loc[stats_df['column_name'] == 'north_displacement_meters', 'column_name'] = "North/South Displacement [m]"
stats_df.loc[stats_df['column_name'] == 'east_displacement_meters', 'column_name'] = "East/West Displacement [m]"

stats_df.loc[stats_df['station'] == '0225', 'station'] = "225 - Kaneohe Bay, HI"
stats_df.loc[stats_df['station'] == '0243', 'station'] = "243 - Nags Head, NC"
# stats_df = stats_df.rename(columns={'vert_displacement_meters': 'Vertical Displacement [m]'})

stats_df = stats_df.rename({
    "count": "Count",
    "min_value": "Min [m]",
    "max_value": "Max [m]",
    "mean": "Mean [m]",
    "std_dev": "Standard Deviation [m]",
}, axis="columns")

# stats_df
```

```{python}
import matplotlib

matplotlib.rcParams["axes.formatter.limits"] = (-99, 99)


def plot_stat(this_stat_df, stat_column, decimals=2, ymax=None):
    plt.figure(figsize=(12, 1.75))
    plt.gca().yaxis.set_major_formatter(
        matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ","))
    )
    ax = sns.barplot(this_stat_df, x="station", y=stat_column, hue="column_name")

    for container in ax.containers:
        ax.bar_label(container, fmt=f"%.{decimals}f", padding=3, fontsize=12)

    plt.xlabel(None)

    if ymax is not None:
        plt.ylim(0, ymax)
    # Move legend below the plot, set to 3 columns, remove box
    plt.legend(
        bbox_to_anchor=(0.5, -0.15), loc="upper center", ncol=3, frameon=False, title=""
    )
```

```{python}
#| label: fig-stats-count
#| fig-cap: "Available Data: Sample Count"

sns.set(font_scale=1.0)

plot_stat(stats_df, 'Count', decimals=0, ymax=300_000_000)
```

```{python}
#| label: fig-stats-range
#| fig-cap: "Available Data: Range [m]"

plot_stat(stats_df, 'Range [m]', ymax=90)
```

```{python}
first_225 = pd.read_parquet("../data/a2_std_partition/station_number=0225/year=2016/month=08/day=26/hour=22/minute=00/data_20160826_2200.parquet")
first_225_timestamp = first_225.index[0]
first_225_timestamp
last_225 = pd.read_parquet("../data/a2_std_partition/station_number=0225/year=2024/month=09/day=11/hour=18/minute=30/data_20240911_1830.parquet")
last_225_timestamp = last_225.index[-1]
last_225_timestamp
first_243 = pd.read_parquet("../data/a2_std_partition/station_number=0243/year=2018/month=08/day=26/hour=15/minute=00/data_20180826_1500.parquet")
first_243_timestamp = first_243.index[0]
first_243_timestamp

last_243 = pd.read_parquet("../data/a2_std_partition/station_number=0243/year=2023/month=07/day=12/hour=23/minute=30/data_20230712_2330.parquet")
last_243_timestamp = last_243.index[-1]
last_243_timestamp

from datetime import datetime

# Create the data
data = {
    'station': ['225', '243'],
    'start_date': [
        first_225_timestamp,
        first_243_timestamp,
    ],
    'end_date': [
        last_225_timestamp,
        last_243_timestamp,
    ]
}

# Create DataFrame
df = pd.DataFrame(data)

# Calculate duration
df['duration'] = df['end_date'] - df['start_date']

# Function to format duration in human readable format
def format_duration(timedelta):
    years = timedelta.days // 365
    remaining_days = timedelta.days % 365
    months = remaining_days // 30
    days = remaining_days % 30

    parts = []
    if years > 0:
        parts.append(f"{years} {'year' if years == 1 else 'years'}")
    if months > 0:
        parts.append(f"{months} {'month' if months == 1 else 'months'}")
    if days > 0:
        parts.append(f"{days} {'day' if days == 1 else 'days'}")

    return ", ".join(parts)

# Add human readable duration
df['duration_human'] = df['duration'].apply(format_duration)

# Format datetime columns to be more readable
df['start_date'] = df['start_date'].dt.strftime('%Y-%m-%d %H:%M')
df['end_date'] = df['end_date'].dt.strftime('%Y-%m-%d %H:%M')

df = df.rename({
    'start_date': "Start Date [UTC]",
    'end_date': "End Date [UTC]",
    'duration_human': "Duration",
}, axis="columns")

```

```{python}
#| label: tbl-duration
#| tbl-cap: Temporal Details of Downloaded CDIP Data


df[['Start Date [UTC]', 'End Date [UTC]', 'Duration']]
```


# Data Cleaning

:::: {.columns}

::: {.column width="45%"}


* Original multi-year data exceeds reasonable training capacity
* Use statistical methods to compute 30 minute statistics
    * MHKiT-Python transforms data to 30-minute statistics
    * Key parameters:
      - Significant wave height ($H_{m_0}$)
      - Energy period ($T_e$)
      - Omnidirectional wave energy flux ($J$)
* Use these statistics to identify unique wave conditions for model training

:::


::: {.column width="55%"}

```{python}
#| eval: false
#| echo: true
#| code-fold: false
#| lst-label: lst-mhkit
#| lst-cap: Calculation of Wave Quantities of Interest from Displacement

import mhkit.wave as wave


def calculate_wave_qoi(input_df, station_number, path):
    input_df = input_df.dropna(axis="index")
    column = "vert_displacement_meters"

    if len(input_df) != 2304:
        return None

    sample_rate_hz = 1.28
    n_fft = 256
    window = "hann"
    detrend = True
    surface_elevation = pd.DataFrame(input_df[column].iloc[:2048])
    spectra = wave.resource.elevation_spectrum(
        surface_elevation, sample_rate_hz, n_fft, window=window, detrend=detrend
    )

    return {
        "time": input_df.index[0],
        "significant_wave_height_meters": wave.resource.significant_wave_height(spectra)["Hm0"].to_list()[0],  # type: ignore
        "energy_period_seconds": wave.resource.energy_period(spectra)["Te"].to_list()[0],  # type: ignore
        "omnidirectional_wave_energy_flux": wave.resource.energy_flux(spectra, np.nan, deep=True)["J"].to_list()[0],  # type: ignore
        "station_number": station_number,
        "path": str(path),
    }

```

:::

:::


# Wave Statistics vs. Time


```{python}
qoi_225 = pd.read_parquet("../data/b2_wave_qoi_stats/qoi_225.parquet")
qoi_243 = pd.read_parquet("../data/b2_wave_qoi_stats/qoi_243.parquet")

```

```{python}
#| label: fig-wave-stats-hm0
#| fig-cap: "Significant Wave Height, $H_{m_0}$ [$m$]"
#| fig-subcap:
#|   - "Wave Energy Test Site - Hawaii"
#|   - "Nags Head - North Carolina"
#| layout-ncol: 2

import matplotlib.pyplot as plt

sns.set(font_scale=0.60)

def compare_qoi_over_time(qoi, label):
    figsize = (5, 1.15)
    xlabel = "Time"
    qoi_225[qoi].plot(
        figsize=figsize, xlabel=None, ylabel=label, linewidth=0.25
    )
    plt.xlabel(None)
    plt.show()

    qoi_243[qoi].plot(
        figsize=figsize, xlabel=None, ylabel=label, linewidth=0.25
    )
    plt.xlabel(None)
    plt.show()

compare_qoi_over_time('significant_wave_height_meters', "$H_{m_0}$ [$m$]")
```

```{python}
#| label: fig-wave-stats-te
#| fig-cap: "Energy Period, $T_e$ [$s$]"
#| fig-subcap:
#|   - "Wave Energy Test Site - Hawaii"
#|   - "Nags Head - North Carolina"
#| layout-ncol: 2

compare_qoi_over_time('energy_period_seconds', "$T_e$ [$s$]")
```

```{python}
#| label: fig-wave-stats-j
#| fig-cap: "Omnidirectional Wave Energy Flux (Wave Power), $J$ [$W/m$]"
#| fig-subcap:
#|   - "Wave Energy Test Site - Hawaii"
#|   - "Nags Head - North Carolina"
#| layout-ncol: 2

compare_qoi_over_time('omnidirectional_wave_energy_flux', "$J$ [$W/m$]")
```

# Wave Classification Dataset Creation Method

:::{.columns}

:::{.column width="40%"}

* Classify data into 30 minute bins:
    * Categorizes data using wave height ($H_{m_0}$) and energy period ($T_e$)
    * Creates sea state matrices to show wave condition frequency
    * Ensures training data covers diverse wave states

* Sample data from bins by count:
  * Use random sampling to pick a single 30 minute dataset from each bin
  * Option to pick more samples

* Yields:
  * Unique 30 minute data bins
  * 76 for CDIP 225 in Kaneohe Bay, Hawaii
  * 112 for CDIP 243 in Nags Head, NC

:::


:::{.column width="60%"}

```{python}
import numpy as np

sns.set(font_scale=0.50)

def plot_wave_heatmap(df, figsize=(6, 2.75)):
    # Create bins for Hm0 and Te
    hm0_bins = np.arange(0, df['significant_wave_height_meters'].max() + 0.5, 0.5)
    te_bins = np.arange(0, df['energy_period_seconds'].max() + 1, 1)

    # Use pd.cut to bin the data
    hm0_binned = pd.cut(df['significant_wave_height_meters'],
                        bins=hm0_bins,
                        labels=hm0_bins[:-1],
                        include_lowest=True)

    te_binned = pd.cut(df['energy_period_seconds'],
                       bins=te_bins,
                       labels=te_bins[:-1],
                       include_lowest=True)

    # Create cross-tabulation of binned data
    counts = pd.crosstab(hm0_binned, te_binned)

    counts = counts.sort_index(ascending=False)


    # Replace 0 counts with NaN
    counts = counts.replace(0, np.nan)

    # Create figure and axis
    plt.figure(figsize=figsize)

    # Create heatmap using seaborn
    ax = sns.heatmap(
        counts,
        cmap='viridis',
        annot=True,  # Add count annotations
        fmt='.0f',   # Format annotations as integers
        cbar_kws={'label': 'Count'},
    )

    # Customize plot
    plt.xlabel('Energy Period Te (s)')
    plt.ylabel('Significant Wave Height Hm0 (m)')

    # Rotate x-axis labels for better readability
    # plt.xticks(rotation=45)
    plt.yticks(rotation=90)

    # Adjust layout to prevent label cutoff
    plt.tight_layout()
    plt.show()

```

```{python}
qoi_225 = pd.read_parquet("../data/b2_wave_qoi_stats/qoi_225.parquet")
qoi_243 = pd.read_parquet("../data/b2_wave_qoi_stats/qoi_243.parquet")
```

```{python}
#| label: fig-wave-stats-225-count
#| fig-cap: "CDIP 225 Sea State Distribution: Wave Height ($H_{m_0}$) vs Energy Period ($T_e$)"

plot_wave_heatmap(qoi_225)
```


```{python}
#| label: fig-wave-stats-243-count
#| fig-cap: "CDIP 243 Sea State Distribution: Wave Height ($H_{m_0}$) vs Energy Period ($T_e$)"

plot_wave_heatmap(qoi_243)
```

:::

:::


# Deep Learning Models - LSTM


:::{.columns}

:::{.column width="40%"}

* Processes 3D displacement through stacked LSTM layers
* Uses dropout regularization for stability
* Final linear layer outputs displacement predictions

:::

:::{.column width="60%"}

```{python}
#| eval: false
#| echo: true
#| code-fold: false
#| lst-label: lst-lstm-model
#| lst-cap: Long Short-Term Memory PyTorch Model

class LSTMModel(WavePredictionModel):
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int = 128,
        num_layers: int = 2,
        dropout: float = 0.2,
        learning_rate: float = 1e-3,
    ):
        super().__init__(input_dim, learning_rate)
        self.save_hyperparameters()  # Save all init parameters

        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.lstm = nn.LSTM(
            input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout
        )
        self.fc = nn.Linear(hidden_dim, input_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        lstm_out, _ = self.lstm(x)
        predictions = self.fc(lstm_out)
        return predictions
```

:::

:::

# Deep Learning Models - Enhanced LSTM

:::{.columns}

:::{.column width="40%"}


* Bidirectional processing captures past/future context
* Skip connections maintain gradient flow
* Layer normalization stabilizes training

:::

:::{.column width="60%"}

```{python}
#| eval: false
#| echo: true
#| code-fold: false
#| lst-label: lst-enh-lstm-model
#| lst-cap: Enhanced Long Short-Term Memory PyTorch Model

class EnhancedLSTMModel(WavePredictionModel):
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int = 128,
        num_layers: int = 2,
        dropout: float = 0.2,
        learning_rate: float = 1e-3,
        bidirectional: bool = True,
    ):
        super().__init__(input_dim, learning_rate)
        self.save_hyperparameters()

        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.bidirectional = bidirectional

        # Input processing
        self.input_layer = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout / 2),
        )

        # Main LSTM layers with skip connections
        self.lstm_layers = nn.ModuleList()
        self.layer_norms = nn.ModuleList()

        lstm_input_dim = hidden_dim
        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim

        for _ in range(num_layers):
            self.lstm_layers.append(
                nn.LSTM(
                    lstm_input_dim,
                    hidden_dim,
                    num_layers=1,
                    batch_first=True,
                    bidirectional=bidirectional,
                    dropout=0,
                )
            )
            self.layer_norms.append(nn.LayerNorm(lstm_output_dim))
            lstm_input_dim = lstm_output_dim

        # Output processing
        self.output_layers = nn.ModuleList(
            [
                nn.Linear(lstm_output_dim, hidden_dim),
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.Linear(hidden_dim // 2, input_dim),
            ]
        )

        self.dropouts = nn.ModuleList(
            [nn.Dropout(dropout) for _ in range(len(self.output_layers))]
        )

        # Skip connection
        self.skip_connection = nn.Linear(input_dim, input_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Store original input for skip connection
        original_input = x

        # Input processing
        x = self.input_layer(x)

        # Process through LSTM layers with residual connections
        for lstm, norm in zip(self.lstm_layers, self.layer_norms):
            residual = x
            x, _ = lstm(x)
            x = norm(x)
            if residual.shape == x.shape:
                x = x + residual

        # Output processing
        for linear, dropout in zip(self.output_layers[:-1], self.dropouts[:-1]):
            residual = x
            x = linear(x)
            x = F.relu(x)
            x = dropout(x)
            if residual.shape == x.shape:
                x = x + residual

        # Final output layer
        x = self.output_layers[-1](x)

        # Add skip connection
        x = x + self.skip_connection(original_input)

        return x

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)
```

:::

:::

# Deep Learning Models - Transformer

:::{.columns}

:::{.column width="40%"}

* Uses self-attention for wave pattern analysis
* Multi-head attention captures relationships across sequence
* Identifies both short-term and long-range dependencies

:::


:::{.column width="60%"}

```{python}
#| eval: false
#| echo: true
#| code-fold: false
#| lst-label: lst-trans-model
#| lst-cap: Transformer PyTorch Model

class TransformerModel(WavePredictionModel):

    def __init__(
        self,
        input_dim: int,
        d_model: int = 128,
        nhead: int = 8,
        num_layers: int = 4,
        dropout: float = 0.2,
        learning_rate: float = 1e-3,
    ):
        super().__init__(input_dim, learning_rate)
        self.save_hyperparameters()  # Save all init parameters

        self.input_projection = nn.Linear(input_dim, d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True,
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer, num_layers=num_layers
        )

        self.output_projection = nn.Linear(d_model, input_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.input_projection(x)
        x = self.transformer_encoder(x)
        return self.output_projection(x)
```

:::

:::


# Training

* Input: 128 Samples
* Output: 128 Samples
* Optimizer: Adam
* Learning Rate: 0.001

| Model Type           | Layers    | Epochs    | Special Features                                                |
| ------------         | --------- | --------- | -----------------                                               |
| Baseline LSTM        | 2         | 25        | Basic implementation                                            |
| Extended LSTM        | 2         | 100       | Same as baseline with longer training                           |
| Deep LSTM            | 4         | 25        | Additional LSTM layers                                          |
| Deeper LSTM          | 6         | 25        | Maximum layer depth tested                                      |
| Enhanced LSTM        | 2         | 25        | Bidirectional processing, skip connections, layer normalization |
| Basic Transformer    | 2         | 25        | Multi-head attention mechanism                                  |
| Enhanced Transformer | 2         | 25        | Additional positional encoding, enhanced feed-forward network   |

: Model Architecture Specifications {#tbl-model-arch .striped}

The full training code can be found in this projects GitHub Repository in the file $\texttt{train\_window\_from\_spec.py}$.

# Training Metrics - Baseline Model

:::{.columns}

:::{.column width="30%"}

* Training Metrics:
  * Mean Absolute Error ("Sum of Absolute Errors")
    * Train
    * Validate

$$
\mathrm{MAE} = \frac{\sum_{i - 1}^n | y_i - x_i|}{n}
$$

Where:

* $y_i$ is the prediction
* $x_i$ is the true value
* $n$ is the number of data points

:::


:::{.column width="70%"}

```{python}
#| label: fig-225-train-baseline
#| fig-cap: "Baseline Model: Training vs. Validation Mean Absolute Error"
#| fig-subcap:
#|   - "CDIP 225"
#|   - "CDIP 243"
#| layout-nrow: 2

sns.set(font_scale=1.00)

df = pd.read_parquet("../training_history/training_history_lstm_20241207_205254.parquet")
df = df.set_index(['epoch'])

fig, axs = plt.subplots(figsize=(12, 2))
df[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel="Epoch", ylabel="Mean Absolute Error")
axs.legend(['Train', "Validate"])
plt.show()

df = pd.read_parquet("../training_history/training_history_lstm_20241207_215544.parquet")
df = df.set_index(['epoch'])

fig, axs = plt.subplots(figsize=(12, 2))
df[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel="Epoch", ylabel="Mean Absolute Error")
axs.legend(['Train', "Validate"])
plt.show()

```

:::

:::

# Training Metrics Comparison

:::{.columns}

:::{.column width="50%"}


**100 Epoch LSTM**

```{python}
#| label: fig-225-train-100-LSTM
#| fig-cap: "100 Epoch LSTM Model: Training vs. Validation Mean Absolute Error"
#| fig-subcap:
#|   - "CDIP 225"
#|   - "CDIP 243"
#| layout-nrow: 2

df = pd.read_parquet("../training_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_100/")
df = df.set_index(['epoch'])

fig, axs = plt.subplots(figsize=(12, 1.5))
df[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel="Epoch", ylabel="MAE")
axs.legend(['Train', "Validate"])
plt.show()

df = pd.read_parquet("../training_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_100/")
df = df.set_index(['epoch'])

fig, axs = plt.subplots(figsize=(12, 1.5))
df[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel="Epoch", ylabel="MAE")
axs.legend(['Train', "Validate"])
plt.show()
```

**Transformer Model**


```{python}
#| label: fig-225-train-transformer
#| fig-cap: "Transformer Model: Training vs. Validation Mean Absolute Error"
#| fig-subcap:
#|   - "CDIP 225"
#|   - "CDIP 243"
#| layout-nrow: 2

df = pd.read_parquet("../training_history/model_transformer.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/")
df = df.set_index(['epoch'])

fig, axs = plt.subplots(figsize=(12, 1.5))
df[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel="Epoch", ylabel="MAE")
axs.legend(['Train', "Validate"])

plt.show()

df = pd.read_parquet("../training_history/model_transformer.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/")
df = df.set_index(['epoch'])
fig, axs = plt.subplots(figsize=(12, 1.5))
df[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel="Epoch", ylabel="MAE")
axs.legend(['Train', "Validate"])

plt.show()
```

**4 Layer LSTM Model**

```{python}
#| label: fig-train-4-layer-LSTM
#| fig-cap: "4 Layer LSTM Model: Training vs. Validation Mean Absolute Error"
#| fig-subcap:
#|   - "CDIP 225"
#|   - "CDIP 243"
#| layout-nrow: 2

df = pd.read_parquet("../training_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_4.EPOCHS_25/")
df = df.set_index(['epoch'])

fig, axs = plt.subplots(figsize=(12, 1.5))
df[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel="Epoch", ylabel="MAE")
axs.legend(['Train', "Validate"])
plt.show()

df = pd.read_parquet("../training_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_4.EPOCHS_25/")
df = df.set_index(['epoch'])

fig, axs = plt.subplots(figsize=(12, 1.5))
df[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel="Epoch", ylabel="MAE")
axs.legend(['Train', "Validate"])
plt.show()
```

:::

:::{.column width="50%"}

**6 Layer LSTM Model**

```{python}
#| label: fig-train-6-layer-LSTM
#| fig-cap: "6 Layer LSTM Model: Training vs. Validation Mean Absolute Error"
#| fig-subcap:
#|   - "CDIP 225"
#|   - "CDIP 243"
#| layout-nrow: 2

df = pd.read_parquet("../training_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_6.EPOCHS_25/")
df = df.set_index(['epoch'])

fig, axs = plt.subplots(figsize=(12, 1.5))
df[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel="Epoch", ylabel="MAE")
axs.legend(['Train', "Validate"])

plt.show()

df = pd.read_parquet("../training_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_6.EPOCHS_25/")
df = df.set_index(['epoch'])

fig, axs = plt.subplots(figsize=(12, 1.5))
df[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel="Epoch", ylabel="MAE")
axs.legend(['Train', "Validate"])
plt.show()
```


**Enhanced Transformer Model**


```{python}
#| label: fig-train-enhanced-trans
#| fig-cap: "Enhanced Transformer Model: Training vs. Validation Mean Absolute Error"
#| fig-subcap:
#|   - "CDIP 225"
#|   - "CDIP 243"
#| layout-nrow: 2

df = pd.read_parquet("../training_history/model_enhanced_transformer.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/")
df = df.set_index(['epoch'])

fig, axs = plt.subplots(figsize=(12, 1.5))
df[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel="Epoch", ylabel="MAE")
axs.legend(['Train', "Validate"])

plt.show()

df = pd.read_parquet("../training_history/model_enhanced_transformer.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/")
df = df.set_index(['epoch'])

fig, axs = plt.subplots(figsize=(12, 1.5))
df[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel="Epoch", ylabel="MAE")
axs.legend(['Train', "Validate"])
plt.show()
```

**Enhanced LSTM Model**


```{python}
#| label: fig-train-enhanced-lstm
#| fig-cap: "Enhanced LSTM Model: Training vs. Validation Mean Absolute Error"
#| fig-subcap:
#|   - "CDIP 225"
#|   - "CDIP 243"
#| layout-nrow: 2

df = pd.read_parquet("../training_history/model_enhanced_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/")
df = df.set_index(['epoch'])

fig, axs = plt.subplots(figsize=(12, 2))
df[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel="Epoch", ylabel="Mean Absolute Error")
axs.legend(['Train', "Validate"])

plt.show()

df = pd.read_parquet("../training_history/model_enhanced_transformer.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/")
df = df.set_index(['epoch'])

fig, axs = plt.subplots(figsize=(12, 2))
df[['train_mae', 'val_mae']].plot(ax=axs, linewidth = 0.85, xlabel="Epoch", ylabel="Mean Absolute Error")
axs.legend(['Train', "Validate"])
```

:::

:::

# Results - Baseline LSTM Timeseries


```{python}
result_stats = []
```

```{python}
from sklearn.metrics import mean_absolute_error, r2_score

def calc_stats(label, station, targets, predictions, column='vert_displacement_meters'):
    return {
		'label': label,
		'station': station,
        'mae': mean_absolute_error(targets[column], predictions[column]),
        'r2': r2_score(targets[column], predictions[column]),
        'correlation': np.corrcoef(targets[column], predictions[column])[0,1]
    }

```

```{python}
def plot_test_section_compared_to_input(index, this_bins_df, this_source, this_targets, this_predictions, n_samples=128):
    # Calculate start and stop indices
    start = index * n_samples
    stop = start + n_samples

    # Get source path and load input data
    source_path = this_source.iloc[index]['Source Path']
    input_df = pd.read_parquet(source_path)

    # Get statistics for the title
    stats = this_bins_df[this_bins_df["path"] == source_path]

    # Create figure
    fig, ax = plt.subplots(figsize=(16, 3))

    # Create index arrays for proper alignment
    input_index = np.arange(n_samples * 2)
    shifted_index = np.arange(n_samples, 2 * n_samples)

    # Plot input data with original index
    ax.plot(input_index, input_df['vert_displacement_meters'].iloc[:n_samples * 2].values,
            linewidth=0.85, label="Input", alpha=0.7)

    # Plot target and prediction with shifted index
    ax.plot(shifted_index,
            this_targets['vert_displacement_meters'].iloc[start:stop].values,
            label="Target", linewidth=0.85)
    ax.plot(shifted_index,
            this_predictions['vert_displacement_meters'].iloc[start:stop].values,
            label="Prediction", linewidth=0.75)

    # Configure plot
    plt.ylabel("Vertical Displacement [m]")
    plt.legend(loc="upper right")
    plt.title(
        f"CDIP {stats['station_number'].item()} - $H_{{m0}}$: {stats['hm0_bin'].item()}, "
        f"$T_{{e}}$: {stats['te_bin'].item()}"
    )

    plt.tight_layout()
    plt.show()

def plot_test_section(index, this_bins_df, this_source, this_targets, this_predictions, n_samples=128):
    # Calculate start and stop indices
    start = index * n_samples
    stop = start + n_samples

    scale_factor = 0.5

    this_targets = this_targets.copy()
    this_predictions = this_predictions.copy()

    this_targets *= scale_factor
    this_predictions *= scale_factor

    # Get source path and load input data
    source_path = this_source.iloc[index]['Source Path']

    # Get statistics for the title
    stats = this_bins_df[this_bins_df["path"] == source_path]

    # Create figure
    fig, ax = plt.subplots(figsize=(16, 3))

    ax.plot(
            this_targets['vert_displacement_meters'].iloc[start:stop].values,
            label="Target", linewidth=0.85, marker=".", markersize=4)
    ax.plot(
            this_predictions['vert_displacement_meters'].iloc[start:stop].values,
            label="Prediction", linewidth=0.75, marker=".", markersize=4)

    # Configure plot
    plt.ylabel("Vertical Displacement [m]")
    plt.legend(loc="upper right")
    plt.title(
        f"CDIP {stats['station_number'].item()} - $H_{{m_0}}$: {stats['hm0_bin'].item()}, "
        f"$T_{{e}}$: {stats['te_bin'].item()}",
        fontsize=18,
    )

    plt.tight_layout()
    plt.show()
```


:::{.columns}

:::{.column width="50%"}


**CDIP 225 - Kaneohe Bay, HI**

```{python}
df_bins = pd.read_parquet("./model_input_spec_225.parquet")
targets = pd.read_parquet("../testing_history/model_lstm.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.targets.lstm_20241207_205411.parquet")
predictions = pd.read_parquet("../testing_history/model_lstm.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.predictions.lstm_20241207_205411.parquet")
sources = pd.read_parquet("../testing_history/model_lstm.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.sources.lstm_20241207_205411.parquet")

result_stats.append(calc_stats("Baseline", "225", targets, predictions))
```

```{python}
#| label: fig-lstm-baseline-test-section-1
#| fig-cap: Baseline LSTM - CDIP 225 - Test Section 1

sns.set(font_scale=1.00)

plot_test_section(3, df_bins, sources, targets, predictions)
```

```{python}
#| label: fig-lstm-baseline-test-section-2
#| fig-cap: Baseline LSTM - CDIP 225 - Test Section 2

plot_test_section(4, df_bins, sources, targets, predictions)
```

```{python}
#| label: fig-lstm-baseline-test-section-3
#| fig-cap: Baseline LSTM - CDIP 225 - Test Section 3

plot_test_section(5, df_bins, sources, targets, predictions)
```

:::

:::{.column width="50%"}


**CDIP 243 - Nags Head, NC**


```{python}
targets = pd.read_parquet("../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.targets.lstm_20241207_215743.parquet")
predictions = pd.read_parquet("../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.predictions.lstm_20241207_215743.parquet")
sources = pd.read_parquet("../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.sources.lstm_20241207_215743.parquet")
df_bins = pd.read_parquet("./model_input_spec_243.parquet")

result_stats.append(calc_stats("Baseline", "243", targets, predictions))
```

```{python}
#| label: fig-lstm-baseline-test-section-1-nags
#| fig-cap: Baseline LSTM - CDIP 243 - Test Section 1



plot_test_section(3, df_bins, sources, targets, predictions)
```

```{python}
#| label: fig-lstm-baseline-test-section-2-nags
#| fig-cap: Baseline LSTM - CDIP 243 - Test Section 2

plot_test_section(4, df_bins, sources, targets, predictions)
```

```{python}
#| label: fig-lstm-baseline-test-section-3-nags
#| fig-cap: Baseline LSTM - CDIP 243 - Test Section 3

plot_test_section(5, df_bins, sources, targets, predictions)
```

:::

:::

:::{.columns}

:::{.column width="50%"}

* 128-sample prediction windows vs. actual wave data

:::

:::{.column width="50%"}

* Compares across wave conditions ($H_{m_0}$, $T_e$)

:::

:::

# Results - Quantitative Visualization

:::{.columns}

:::{.column width="50%"}

**Baseline Model**

```{python}
#| label: fig-lstm-baseline-test-all
#| fig-cap: Baseline LSTM - All Test Sections, Train Blue, Test Orange
#| fig-subcap:
#|   - CDIP 225
#|   - CDIP 243
#| layout-nrow: 2

targets = pd.read_parquet("../testing_history/model_lstm.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.targets.lstm_20241207_205411.parquet")
predictions = pd.read_parquet("../testing_history/model_lstm.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.predictions.lstm_20241207_205411.parquet")
sources = pd.read_parquet("../testing_history/model_lstm.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.sources.lstm_20241207_205411.parquet")
df_bins = pd.read_parquet("./model_input_spec_225.parquet")

targets *= 0.3
predictions *= 0.3


targets['vert_displacement_meters'].plot(figsize=(14, 2.25), label="target", linewidth = 0.85)
predictions['vert_displacement_meters'].plot(label="prediction", linewidth=0.75)
plt.ylabel("Vertical Displacement [m]")
plt.legend(labels=["Train", "Test"])
plt.show()

targets = pd.read_parquet("../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.targets.lstm_20241207_215743.parquet")
predictions = pd.read_parquet("../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.predictions.lstm_20241207_215743.parquet")
sources = pd.read_parquet("../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.sources.lstm_20241207_215743.parquet")
df_bins = pd.read_parquet("./model_input_spec_243.parquet")

targets *= 0.3
predictions *= 0.3


targets['vert_displacement_meters'].plot(figsize=(14, 2.25), label="target", linewidth = 0.85)
predictions['vert_displacement_meters'].plot(label="prediction", linewidth=0.75)
plt.ylabel("Vertical Displacement [m]")
plt.show()
```

**Transformer Model**

```{python}
#| label: fig-transformer-test-all
#| fig-cap: Transformer - All Test Sections
#| fig-subcap:
#|   - CDIP 225
#|   - CDIP 243
#| layout-nrow: 2

targets = pd.read_parquet("../testing_history/model_transformer.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.targets.transformer_20241207_231033.parquet")
predictions = pd.read_parquet("../testing_history/model_transformer.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.predictions.transformer_20241207_231033.parquet")
sources = pd.read_parquet("../testing_history/model_transformer.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.sources.transformer_20241207_231033.parquet")

df_bins = pd.read_parquet("./model_input_spec_225.parquet")


targets['vert_displacement_meters'].plot(figsize=(14, 2.25), label="target", linewidth = 0.85)
predictions['vert_displacement_meters'].plot(label="prediction", linewidth=0.75)
plt.ylabel("Vertical Displacement [m]")
plt.show()
result_stats.append(calc_stats("Transformer", "225", targets, predictions))

targets = pd.read_parquet("../testing_history/model_transformer.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.targets.transformer_20241207_235408.parquet")
predictions = pd.read_parquet("../testing_history/model_transformer.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.predictions.transformer_20241207_235408.parquet")
sources = pd.read_parquet("../testing_history/model_transformer.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_25/test_results.sources.transformer_20241207_235408.parquet")

df_bins = pd.read_parquet("./model_input_spec_243.parquet")


targets['vert_displacement_meters'].plot(figsize=(14, 2.25), label="target", linewidth = 0.85)
predictions['vert_displacement_meters'].plot(label="prediction", linewidth=0.75)
plt.ylabel("Vertical Displacement [m]")
plt.show()

result_stats.append(calc_stats("Transformer", "243", targets, predictions))
```



:::

:::{.column width="50%"}

**LSTM 100 Epoch Model**

```{python}
#| label: fig-transformer-lstm-100
#| fig-cap: LSTM 100 Epoch - All Test Sections
#| fig-subcap:
#|   - CDIP 225
#|   - CDIP 243
#| layout-nrow: 2

targets = pd.read_parquet("../testing_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_100/test_results.targets.lstm_20241208_023701.parquet")
predictions = pd.read_parquet("../testing_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_100/test_results.predictions.lstm_20241208_023701.parquet")
sources = pd.read_parquet("../testing_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_100/test_results.sources.lstm_20241208_023701.parquet")

df_bins = pd.read_parquet("./model_input_spec_225.parquet")


targets['vert_displacement_meters'].plot(figsize=(14, 2.25), label="target", linewidth = 0.85)
predictions['vert_displacement_meters'].plot(label="prediction", linewidth=0.75)
plt.ylabel("Vertical Displacement [m]")
plt.show()
result_stats.append(calc_stats("100 Epoch LSTM", "225", targets, predictions))

targets = pd.read_parquet("../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_100/test_results.targets.lstm_20241208_061107.parquet")
predictions = pd.read_parquet("../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_100/test_results.predictions.lstm_20241208_061107.parquet")
sources = pd.read_parquet("../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_2.EPOCHS_100/test_results.sources.lstm_20241208_061107.parquet")

df_bins = pd.read_parquet("./model_input_spec_243.parquet")


targets['vert_displacement_meters'].plot(figsize=(14, 2.25), label="target", linewidth = 0.85)
predictions['vert_displacement_meters'].plot(label="prediction", linewidth=0.75)
plt.ylabel("Vertical Displacement [m]")
plt.show()
result_stats.append(calc_stats("100 Epoch LSTM", "243", targets, predictions))

```

**LSTM 4 Layers Model**

```{python}
#| label: fig-transformer-lstm-4-layer
#| fig-cap: LSTM 4 Layer - All Test Sections
#| fig-subcap:
#|   - CDIP 225
#|   - CDIP 243
#| layout-nrow: 2

targets = pd.read_parquet("../testing_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_4.EPOCHS_25/test_results.targets.lstm_20241208_072749.parquet")
predictions = pd.read_parquet("../testing_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_4.EPOCHS_25/test_results.predictions.lstm_20241208_072749.parquet")
sources = pd.read_parquet("../testing_history/model_lstm.station_number_225.window_128.hidden_dim_128.NUM_LAYERS_4.EPOCHS_25/test_results.sources.lstm_20241208_072749.parquet")

df_bins = pd.read_parquet("./model_input_spec_225.parquet")


targets['vert_displacement_meters'].plot(figsize=(14, 2.25), label="target", linewidth = 0.85)
predictions['vert_displacement_meters'].plot(label="prediction", linewidth=0.75)
plt.ylabel("Vertical Displacement [m]")
plt.show()
result_stats.append(calc_stats("4 Layer LSTM", "225", targets, predictions))

targets = pd.read_parquet("../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_4.EPOCHS_25/test_results.targets.lstm_20241208_083208.parquet")
predictions = pd.read_parquet("../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_4.EPOCHS_25/test_results.predictions.lstm_20241208_083208.parquet")
sources = pd.read_parquet("../testing_history/model_lstm.station_number_243.window_128.hidden_dim_128.NUM_LAYERS_4.EPOCHS_25/test_results.sources.lstm_20241208_083208.parquet")

df_bins = pd.read_parquet("./model_input_spec_243.parquet")


targets['vert_displacement_meters'].plot(figsize=(14, 2.25), label="target", linewidth = 0.85)
predictions['vert_displacement_meters'].plot(label="prediction", linewidth=0.75)
plt.ylabel("Vertical Displacement [m]")
plt.show()
result_stats.append(calc_stats("4 Layer LSTM", "243", targets, predictions))

```

:::

:::


# Results Comparison - MAE

```{python}
results_df = pd.DataFrame(result_stats)
```

```{python}
#| label: fig-results-mae-comparison
#| fig-cap: Mean Absolute Error Comparison by Model (Lower is better)

results_df = results_df.sort_values(["mae", "station"])

plt.figure(figsize=(8, 3))
sns.barplot(results_df, y="label", x="mae", hue="station")
for i in plt.gca().containers:
    plt.bar_label(i, fmt='%.2f', padding=3)
plt.ylabel(None);
plt.xlabel("Mean Absolute Error");
```

```{python}
#| label: fig-results-mae-comparison-by-station
#| fig-cap: Mean Absolute Error Comparison by Station (Lower is better)

plt.figure(figsize=(8, 3.0))
sns.barplot(results_df, y="station", x="mae", hue="label")
for i in plt.gca().containers:
    plt.bar_label(i, fmt='%.2f', padding=3)
plt.ylabel(None);
plt.xlabel("Mean Absolute Error");
plt.legend(bbox_to_anchor=(1.25, 1.0),
          loc='upper center',
          ncol=1,
          frameon=False, title="Model")
```


# Results - $R^2$

```{python}
#| label: fig-results-r2-comparison
#| fig-cap: $R^2$ Comparison by Model

results_df = results_df.sort_values(["r2"], ascending=False)

plt.figure(figsize=(8, 3.0))
sns.barplot(results_df, y="label", x="r2", hue="station")
for i in plt.gca().containers:
    plt.bar_label(i, fmt='%.2f', padding=3)
plt.ylabel(None);
plt.xlabel("$R^2$");
```

```{python}
#| label: fig-results-r2-comparison-by-station
#| fig-cap: $R^2$ Comparison by Station

plt.figure(figsize=(8, 3.0))
sns.barplot(results_df, y="station", x="r2", hue="label")
for i in plt.gca().containers:
    plt.bar_label(i, fmt='%.2f', padding=3)
plt.ylabel(None);
plt.xlabel("$R^2$");
plt.legend(bbox_to_anchor=(1.25, 1.0),
          loc='upper center',
          ncol=1,
          frameon=False, title="Model")
```


# Pearson's Correlation [$\rho$]

```{python}
#| label: fig-results-correlation-comparison
#| fig-cap: Pearson's Correlation Comparison by Model

results_df = results_df.sort_values(["correlation"], ascending=False)

plt.figure(figsize=(8, 3))
sns.barplot(results_df, y="label", x="correlation", hue="station")
for i in plt.gca().containers:
    plt.bar_label(i, fmt='%.2f', padding=3)
plt.ylabel(None);
plt.xlabel("Correlation");
```

```{python}
#| label: fig-results-correlation-comparison-by-station
#| fig-cap: Pearson's Correlation Comparison by Station

plt.figure(figsize=(8, 3.0))
sns.barplot(results_df, y="station", x="correlation", hue="label")
for i in plt.gca().containers:
    plt.bar_label(i, fmt='%.2f', padding=3)
plt.ylabel(None);
plt.xlabel("Correlation");
plt.legend(bbox_to_anchor=(1.25, 1.0),
          loc='upper center',
          ncol=1,
          frameon=False, title="Model")
```

# Results Summary

:::{.columns}

:::{.column width="50%"}

**Model Performance Metrics:**

* Evaluation using MAE, $R^2$, and correlation ($\rho$)
* 4-layer LSTM achieved best results:
  - WETS: MAE = $0.50\,\mathrm{m}$, $R^2$ = 0.89, $\rho$ = 0.94
  - Nags Head: MAE = $0.87\,\mathrm{m}$, $R^2$ = 0.87, $\rho$ = 0.93

:::

:::{.column width="50%"}

**Key Findings:**

* LSTM models outperformed Transformers consistently
* Increased training epochs improved baseline LSTM
* WETS predictions more accurate than Nags Head
* Enhanced architectures showed no additional benefits

:::

:::

# Conclusion

:::{.columns}

:::{.column width="50%"}

**Project Overview:**

* Built deep learning models for wave prediction at two sites
* Processed and sampled data to capture diverse wave states
* Evaluated using MAE, $R^2$, and correlation metrics

**Key Results:**

* 4-layer LSTM performed best (WETS: MAE = $0.50\,\mathrm{m}$)
* Better accuracy at WETS vs. Nags Head
* Added complexity showed diminishing returns

:::

:::{.column width="50%"}

**Technical Insights:**

* LSTMs excel at capturing wave patterns
* Transformers struggled with wave dynamics
* Extended training improved performance
* Structured sampling proved effective

**Future Directions:**

* Extend to north/east displacement predictions
* Incorporate physics-informed neural networks
* Test at additional CDIP buoys
* Optimize for real-time applications
* Test a unified model

:::

:::

# Thank You!

# References
